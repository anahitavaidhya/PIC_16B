[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Anahita",
    "section": "",
    "text": "Anahita Vaidhya is a passionate UCLA student majoring in Cognitive Science with a focus on computing, combining strong technical skills with a keen interest in product management and AI. Anahita thrives at the intersection of technology, business, and human-centered design, bringing a growth mindset and leadership experience to every project she undertakes."
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html",
    "href": "posts/HW5_ImageClassification/index.html",
    "title": "Image Classification",
    "section": "",
    "text": "Welcome to Python for Beginners! In this blog post, we will be diving into the world of image classification using deep learning!\nOur Goal: Build machine learning model in Keras that will be able to distinguish between cats and dogs!\nWe‚Äôll walk through everything step by step ‚Äî starting from loading and preprocessing the dataset to training a CNN (convolutional neural network) that classifies images as either a cat or a dog. Along the way, we‚Äôll explore key concepts in computer vision, including convolutional layers, pooling, and data augmentation.\nIt‚Äôs going to be a crazy ride, so grab your umbrella - because without the right gear, it might just feel like it‚Äôs raining cats and dogs! ‚òîüêæ\n\n\n\n\n\nAs always, we‚Äôre going to start off with importing all the necessary libraries.\n\nimport os\nimport keras\nfrom keras import datasets, layers, models\nimport tensorflow as tf\nfrom keras import utils\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n\n\n\n\nWe got the following dataset from Kaggle that gives us labeled images of cats and dogs to work with.\nTo work with the dataset from TFDS (TensorFlow Datasets), we can load it and split it into 40% training, 10% validation, and 10% test sets:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% training, 10% validation, and 10% test\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    # Dataset is returned as (image, label) pairs\n    as_supervised=True,\n)\n\n# Check number of samples in each split\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\nThis next block contains code that helps us resize the images (of different shapes and sizes) to a fixed size of 150x150.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\n\n\nAnd finally, we will be optimizing our data for it to better work with our deep learning models, especially when we are working with large datasets.\n.batch(batch_size): groups images in groups of ‚Äúbatch_size‚Äù (64 in this case) which in turn improves memory efficiency and parallel processing\n.prefetch(tf_data.AUTOTUNE): allows the model to ‚Äúfetch‚Äù the next batch of images, all while processing the current batch\n.cache(): if possible, stores dataset in memory\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\n\n\nIn this next part, we will be writing a function to get used to working with datasets. In this function, we have to create a two-row visualization where the first row shows images of three random cats and the second row shows images of three random dogs.\nFirst, we‚Äôll have to seperate the dataset into two: images and labels. From there, we will use the label to figure out if it is an image of a dog or a cat. Then, it is easy to seperate and plot the two.\n\ndef show_images(train_ds):\n  # Convert dataset into 2 lists\n  image_list = []\n  label_list = []\n\n  # Iterate through small batch of images/labels\n  for image, label in train_ds.take(1).as_numpy_iterator():\n      image_list.append(image)\n      label_list.append(label)\n\n  # Convert lists into NumPy arrays\n  image_array = np.array(image_list)\n  label_array = np.array(label_list)\n\n  # Split images into dogs and cats\n  cats = image_array[label_array == 0]\n  dogs = image_array[label_array == 1]\n\n  # Pick 3 random cat and dog images\n  num_samples = min(len(cats), len(dogs), 3)\n\n  random_cats = random.sample(list(cats), num_samples)\n  random_dogs = random.sample(list(dogs), num_samples)\n\n  # Display\n  fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n\n  # 3 cat images in the first row\n  for i in range(3):\n      axes[0, i].imshow(random_cats[i] / 255.0)\n      axes[0, i].axis(\"off\")\n      axes[0, i].set_title(\"Cat\")\n\n  # 3 dog images in the second row\n  for i in range(3):\n      axes[1, i].imshow(random_dogs[i] / 255.0)\n      axes[1, i].axis(\"off\")\n      axes[1, i].set_title(\"Dog\")\n\n  # Show\n  plt.tight_layout()\n  plt.show()\n\n\nshow_images(train_ds)\n\n\n\n\n\n\n\n\nAwww‚Ä¶how cute!\n\n\n\n\n\nBefore we train our model, we want to analyze our datasets label distribution, which is, how many dogs and cats are there? This line of code extracts the labels from the dataset.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nWith that information, we can print out the number of cats and dogs in our dataset to make sure there‚Äôs not an imbalance between the two.\n\nlabels = np.array(list(labels_iterator))\n\nnum_cats = np.sum(labels == 0)\nnum_dogs = np.sum(labels == 1)\n\nprint(f\"Number of cats: {num_cats}\")\nprint(f\"Number of dogs: {num_dogs}\")\n\nNumber of cats: 4637\nNumber of dogs: 4668\n\n\n\n\n\nA baseline model is the simplest model that makes predictions, without directly learning from the data. The baseline tends to predict the ‚Äúmajority class‚Äù so if there were 60% cats and 40% dogs, it would guess cats for 60% of the pictures.\nSince our model is around a 50/50 split, we would predict the baseline model accuracy to be around 50%!\n\ntotal = num_cats + num_dogs\nbaseline = (max(num_cats, num_dogs) / total) * 100\nprint(f\"Baseline accuracy: {baseline:.2f}%\")\n\nBaseline accuracy: 50.17%\n\n\nWe were right!\n\n\n\n\nNow, let‚Äôs get to the good stuff. We will start off by building a simple Keras Sequential model to classify images of cats and dogs. We want to start off with at least:\n\n2 Conv2D layers: detects small/complex (based on filter size) patterns, like edges and textures, in the image using 32 (64, 128, etc.) filters of size 3x3.\n2 MaxPooling2D layers: reduces the image size by taking the most important features from a 2x2 region to make the model more efficient\n1 Flatten layer: converts 2D image data into a 1D array to prepare it for the fully connected layers\n1 Dropout layer: randomly turns off 50% (or however any) of neurons to prevent overfitting and improve generalization\n1 Dense layer: outputs a probability between 0 and 1 which determines whether the image is a cat or a dog\n\nWe will play around with the parameters until we get at least 55% validation accuracy!\n\n\n\nmodel1 = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n\n\nThe compile() function essentially configures how the model learns! * optimizer=‚Äòadam‚Äô: adjusts weights efficiently to minimize errors during training (adam = Adaptive Moment Estimation) * loss=‚Äúbinary_crossentropy:‚Äù measures how well the model distinguishes between the two classes (cats vs.¬†dogs) * metrics=[‚Äòaccuracy‚Äô]: tracks the percentage of correctly classified images\n\nmodel1.compile(optimizer='adam',\n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])\n\n\n\n\nThe fit() function uses the training data to train the model and validates it using the validation data! * train_ds: this is the dataset that is used for training the model, where the model learns patterns * epochs=20: the model will iterate through the training data 20 times to adjust weights and minimize errors * validation_data=validation_ds: evaluates the model‚Äôs performance after each epoch using the validation dataset which helps track how well the model generalizes to new, unseen data\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18s 74ms/step - accuracy: 0.5214 - loss: 103.9403 - val_accuracy: 0.5327 - val_loss: 0.6821\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.5922 - loss: 0.6596 - val_accuracy: 0.5503 - val_loss: 0.7064\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.6649 - loss: 0.5952 - val_accuracy: 0.5456 - val_loss: 0.7385\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.7212 - loss: 0.5119 - val_accuracy: 0.5641 - val_loss: 0.9743\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 33ms/step - accuracy: 0.7778 - loss: 0.4484 - val_accuracy: 0.5838 - val_loss: 0.8604\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8155 - loss: 0.3833 - val_accuracy: 0.5838 - val_loss: 1.0832\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8565 - loss: 0.3123 - val_accuracy: 0.5632 - val_loss: 1.5482\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8870 - loss: 0.2848 - val_accuracy: 0.5701 - val_loss: 1.4691\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8932 - loss: 0.2519 - val_accuracy: 0.5825 - val_loss: 1.7660\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9152 - loss: 0.2185 - val_accuracy: 0.5851 - val_loss: 1.6861\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 32ms/step - accuracy: 0.9323 - loss: 0.1865 - val_accuracy: 0.5920 - val_loss: 1.5458\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9348 - loss: 0.1822 - val_accuracy: 0.6045 - val_loss: 1.7919\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 32ms/step - accuracy: 0.9409 - loss: 0.1656 - val_accuracy: 0.5972 - val_loss: 1.8326\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9403 - loss: 0.1635 - val_accuracy: 0.5903 - val_loss: 1.9949\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9279 - loss: 0.2091 - val_accuracy: 0.6195 - val_loss: 1.7677\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9546 - loss: 0.1325 - val_accuracy: 0.6182 - val_loss: 1.8156\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9562 - loss: 0.1256 - val_accuracy: 0.6191 - val_loss: 1.9112\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 32ms/step - accuracy: 0.9640 - loss: 0.1083 - val_accuracy: 0.6238 - val_loss: 1.7145\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9725 - loss: 0.0838 - val_accuracy: 0.6230 - val_loss: 2.0231\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9718 - loss: 0.0929 - val_accuracy: 0.6178 - val_loss: 1.8647\n\n\n\n\n\nWe plot training and validation accuracy data to help us see what is going on with the model. In order to analyze correctly, here is what to look out for:\n\nTraining Accuracy Curve: this line shows how well the model performs on the data it was trained on. It usually improves over time as the model learns.\nValidation Accuracy Curve: this line shows how well the model generalizes to unseen data (validation set). Ideally, this should also increase, but it can sometimes behave differently.\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn our ‚ÄúTraining vs.¬†Validation Accuracy‚Äù graph, we can observe that the training accuracy increases and starts to plateau slowly. Our validation accuracy is significantly lower and remains more or less a flat line throughout, with a slight increase.\nWhat This Means: * Training accuracy higher than validation: indicates that the model is learning well on the training data, but is struggling to generalize to the validation set (a sign of overfitting if the gap widens over time). * Validation accuracy higher than training: this can happen in early training, especially if the model is underfitting (not complex enough). It usually indicates the model hasn‚Äôt fully learned the training data yet.\n\n\n\nIn our ‚ÄúTraining vs.¬†Validation Loss‚Äù graph, we can observe that the training loss quickly drops from high to low, forming an L-shape, while the validation loss remains flatter throughout, with a slight increase.\nWhat This Means: * Validation loss higher than training: his can happen in the early stages of training, especially if the model is underfitting (not complex enough or not learning enough from the data). It typically indicates that the model hasn‚Äôt yet learned to capture the patterns in the training data effectively. Over time, as the model improves, the validation loss should ideally decrease, but if it stays high while training loss continues to decrease, it could indicate a generalization issue. * Training loss higher than validation: this indicates that the model isn‚Äôt learning as efficiently from the training data as it should, and it may need more training time or a more complex model to improve performance.\n\n\n\n\nAfter training the model, it‚Äôs important to see how well the model performs on unseen test data. This is done using the evaluate() method:\n\ntest_loss, test_acc = model1.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 77ms/step - accuracy: 0.5952 - loss: 1.8894\nTest Accuracy: 60.40%\n\n\n\nval_loss, val_acc = model1.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 10ms/step - accuracy: 0.6166 - loss: 1.9826\nValidation Accuracy: 61.78%\n\n\n\n\n\nThe validation accuracy of my model stabilized to 60.58% during training.\nAs calculated earlier, the baseline accuracy was coming out to be around 50%. Our model, with its 61.78% accuracy, is doing better than baseline by around 10%, which shows an improvement in generalization.\nYes, since our training accuracy exceeds validation accuracy, we observe overfitting in our model.\n\n\n\n\n\n\nTo get our model to differentiate between images of cats and dogs better, we will be adding data augmentation layers to our model. What data augmentation layers do is create transformed versions of the input images during training to help the model generalize better. These layers apply random modifications to the images such as flipping and rotating to increase data diversity without needing more labeled images.\n\n\nWe will start of with creating a keras.layers.RandomFlip() layer. We can have this flip the image horizontally, vertically, or both, by specifying with mode. For this example, we will set mode = ‚Äúhorizontal‚Äù which will flip the images along the horizontal axis.\n\nflipped = layers.RandomFlip(mode = \"horizontal\")\n\n\nsample_image, _ = next(iter(train_ds.unbatch().take(1)))\n\n\n# Plot original + 4 flipped versions\nplt.figure(figsize=(10, 5))\n\n# Original image\nplt.subplot(1, 5, 1)\nplt.imshow(sample_image.numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nplt.axis(\"off\")\n\n# Apply RandomFlip multiple times\nfor i in range(4):\n    flipped_image = flipped(sample_image)\n    plt.subplot(1, 5, i + 2)\n    plt.imshow(np.array(flipped_image).astype(\"uint8\"))\n    plt.title(f\"Flip {i+1}\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nTransformations like flipping, rotating, and cropping are applied randomly to each image during training. This means that every time the model runs, the augmented versions of images may look different.\n\n\n\nNext, we will make a keras.layers.RandomRotation() layer which will randomly rotate the image during training and helps the model be better when there are variations in object orientation.\n\nrandomroto = layers.RandomRotation(0.2)\n\n\nsample_image, _ = next(iter(train_ds.unbatch().take(1)))  # Take a single image\n\n\n# Plot original + 4 flipped versions\nplt.figure(figsize=(10, 5))\n\n# Original image\nplt.subplot(1, 5, 1)\nplt.imshow(sample_image.numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nplt.axis(\"off\")\n\n# Apply RandomFlip multiple times\nfor i in range(4):\n    flipped_image = randomroto(sample_image)  # Apply RandomFlip\n    plt.subplot(1, 5, i + 2)\n    plt.imshow(np.array(flipped_image).astype(\"uint8\"))\n    plt.title(f\"Flip {i+1}\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNow, we‚Äôll integrate the data augmentation layers into our model we created above to see if these new layers improve the model‚Äôs ability to distinguish between cats and dogs. By adding random flipping and rotation, we expose the model to more variations in image orientation which in turn helps it learn more generalizable features rather than memorizing specific image patterns to prevent overfitting and improve validation accuracy.\n\n\n\nmodel2 = keras.Sequential([\n    # Data augmentation layers\n    flipped,\n    randomroto,\n\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n\n\n# Compile the model\nmodel2.compile(optimizer=\"adam\",\n               loss=\"binary_crossentropy\",\n               metrics=[\"accuracy\"]\n              )\n\n\n\n\n\n# Train the model for 20 epochs\nhistory2 = model2.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds\n                     )\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11s 44ms/step - accuracy: 0.5437 - loss: 27.9833 - val_accuracy: 0.6131 - val_loss: 0.6547\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.5951 - loss: 0.6623 - val_accuracy: 0.6290 - val_loss: 0.6451\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6008 - loss: 0.6656 - val_accuracy: 0.6341 - val_loss: 0.6394\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6237 - loss: 0.6548 - val_accuracy: 0.6599 - val_loss: 0.6256\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6290 - loss: 0.6462 - val_accuracy: 0.6647 - val_loss: 0.6119\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6346 - loss: 0.6378 - val_accuracy: 0.6660 - val_loss: 0.6300\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6141 - loss: 0.6599 - val_accuracy: 0.6582 - val_loss: 0.6340\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.6373 - loss: 0.6422 - val_accuracy: 0.7094 - val_loss: 0.5763\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6510 - loss: 0.6365 - val_accuracy: 0.7158 - val_loss: 0.5737\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.6490 - loss: 0.6312 - val_accuracy: 0.6707 - val_loss: 0.6139\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6389 - loss: 0.6325 - val_accuracy: 0.6999 - val_loss: 0.5846\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6771 - loss: 0.6134 - val_accuracy: 0.6814 - val_loss: 0.6020\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6574 - loss: 0.6226 - val_accuracy: 0.7111 - val_loss: 0.5728\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6913 - loss: 0.5886 - val_accuracy: 0.7137 - val_loss: 0.5677\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6877 - loss: 0.5840 - val_accuracy: 0.6986 - val_loss: 0.5988\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6931 - loss: 0.5873 - val_accuracy: 0.7390 - val_loss: 0.5598\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 41ms/step - accuracy: 0.6969 - loss: 0.5830 - val_accuracy: 0.7098 - val_loss: 0.5699\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.7116 - loss: 0.5680 - val_accuracy: 0.7472 - val_loss: 0.5383\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.7123 - loss: 0.5629 - val_accuracy: 0.7485 - val_loss: 0.5332\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.7058 - loss: 0.5668 - val_accuracy: 0.7347 - val_loss: 0.5296\n\n\n\n\n\n\nSame principles as above apply here to analyze our plots.\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history2.history[\"accuracy\"], label = \"Training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history2.history[\"loss\"], label = \"Training\")\nplt.plot(history2.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\nTraining vs.¬†Validation Accuracy * Both training and validation accuracy show an upward trend, which indicates that the model is learning over time. * The validation accuracy closely follows the training accuracy, which indicates minimal overfitting and a good generalization to unseen data. * Accuracy reaches around 62%, which is an improvement from the baseline.\nTraining vs.¬†Validation Loss * The training loss starts very high but drops instantly, forming an ‚ÄúL‚Äù shape. * The validation loss remains flat throughout and much lower than the training loss, which is unusual.\nOverall * The model improves in accuracy, showing successful learning. * There is no extreme overfitting, as validation and training accuracy remain close.\n\ntest_loss, test_acc = model2.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 12ms/step - accuracy: 0.7194 - loss: 0.5441\nTest Accuracy: 73.04%\n\n\n\nval_loss, val_acc = model2.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 14ms/step - accuracy: 0.7368 - loss: 0.5196\nValidation Accuracy: 73.47%\n\n\n\n\n\n\nThe validation accuracy of this model is 73.47%.\nThis validation accuracy of 73.47% shows great improvement from the previous model accuracy of 61.78%. There is an increase of around 12%.\nThere isn‚Äôt any severe overfitting, as validation accuracy does not lag far behind training accuracy.\n\n\n\n\n\nTo make our model better and more efficient, it is good to apply simple transformations to the input data before training. This is called preprocessing.\nIn this case, our image pixels have RGB values ranging from 0 to 255, but many models train more efficiently when these values are normalized between 0 and 1 (or -1 and 1).\nThe following code block shows us how to normalize our values:\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\n\n\nmodel3 = keras.Sequential([\n    # Preprocessor layer\n    preprocessor,\n\n    flipped,\n    randomroto,\n\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n\n\n# Compile the model\nmodel3.compile(optimizer=\"adam\",\n               loss=\"binary_crossentropy\",\n               metrics=[\"accuracy\"]\n              )\n\n\n\n\n\n# Train the model for 20 epochs\nhistory3 = model3.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds\n                     )\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 41ms/step - accuracy: 0.5534 - loss: 0.9582 - val_accuracy: 0.6496 - val_loss: 0.6330\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6345 - loss: 0.6363 - val_accuracy: 0.7167 - val_loss: 0.5682\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.6792 - loss: 0.5959 - val_accuracy: 0.7240 - val_loss: 0.5391\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6998 - loss: 0.5779 - val_accuracy: 0.7339 - val_loss: 0.5277\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.7166 - loss: 0.5581 - val_accuracy: 0.7244 - val_loss: 0.5331\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7241 - loss: 0.5464 - val_accuracy: 0.7567 - val_loss: 0.5009\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7353 - loss: 0.5260 - val_accuracy: 0.7554 - val_loss: 0.4939\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7376 - loss: 0.5330 - val_accuracy: 0.7601 - val_loss: 0.5035\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7470 - loss: 0.5116 - val_accuracy: 0.7635 - val_loss: 0.4777\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7613 - loss: 0.5041 - val_accuracy: 0.7837 - val_loss: 0.4631\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7628 - loss: 0.4971 - val_accuracy: 0.7760 - val_loss: 0.4826\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7608 - loss: 0.4898 - val_accuracy: 0.7825 - val_loss: 0.4571\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7730 - loss: 0.4863 - val_accuracy: 0.7696 - val_loss: 0.4870\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.7760 - loss: 0.4797 - val_accuracy: 0.7846 - val_loss: 0.4788\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7827 - loss: 0.4714 - val_accuracy: 0.7979 - val_loss: 0.4638\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.7794 - loss: 0.4621 - val_accuracy: 0.7945 - val_loss: 0.4559\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7782 - loss: 0.4673 - val_accuracy: 0.8014 - val_loss: 0.4578\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.7852 - loss: 0.4618 - val_accuracy: 0.7902 - val_loss: 0.4658\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7970 - loss: 0.4571 - val_accuracy: 0.7898 - val_loss: 0.4626\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7964 - loss: 0.4507 - val_accuracy: 0.8018 - val_loss: 0.4494\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history3.history[\"accuracy\"], label = \"Training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history3.history[\"loss\"], label = \"Training\")\nplt.plot(history3.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\nTraining vs.¬†Validation Accuracy\n\nThe validation accuracy and training accuracy both show a steady increase close together, eventually stabilizing close to around 80%.\nThe validation accuracy remains slightly higher than the training accuracy, which means there is good generalization without much overfitting.\nThe model performs well on unseen data, which means it is learning effectively.\n\nTraining vs.¬†Validation Loss\n\nBoth training and validation loss decrease over time, which is a sign of successful learning.\nThe curves are close, meaning the model is not overfitting and maintains a good balance between training and validation performance.\n\n\ntest_loss, test_acc = model3.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 11ms/step - accuracy: 0.7783 - loss: 0.4538\nTest Accuracy: 79.36%\n\n\n\n# Evaluate on validation dataset\nval_loss, val_acc = model3.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 11ms/step - accuracy: 0.7957 - loss: 0.4574\nValidation Accuracy: 80.18%\n\n\n\n\n\nThe validation accuracy of our model with an additional layer of preprocessing reaches 80.18%.\nThe validation accuracy of this model is a significant increase from model 1 from 61.78% accuracy to 80.18% accuracy, an increase of around 19%.\nThere is no significant overfitting observed in this model, as the validation accuracy closely follows the training accuracy, indicating good generalization to unseen data.\n\n\n\n\n\n\nWhat we have been doing thus far is training models from scratch to distinguish between cats and dogs. But what if we could build on existing knowledge instead of starting fresh? That‚Äôs where transfer learning comes in.\nRather than training a model from the ground up, we can use a pretrained base model, which is a model that has already learned useful features from a massive dataset. For image classification tasks, models like MobileNetV3Large have been trained on millions of images and can recognize patterns like edges, textures, and shapes. By incorporating such a model into our workflow, we can take advantage of these learned features and fine-tune them for our specific task.\nThe following code downloads MobileNetV3Large and configures it as a layer in our model:\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 0us/step\n\n\n\n\n\nmodel4 = keras.Sequential([\n    flipped,\n    randomroto,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\n\n\n\n\nmodel4.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy']\n               )\n\n\n\n\n\nhistory4 = model4.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18s 74ms/step - accuracy: 0.6939 - loss: 3.4024 - val_accuracy: 0.9540 - val_loss: 0.2699\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 17s 54ms/step - accuracy: 0.8867 - loss: 0.8977 - val_accuracy: 0.9678 - val_loss: 0.1746\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 55ms/step - accuracy: 0.9043 - loss: 0.7033 - val_accuracy: 0.9712 - val_loss: 0.1501\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9139 - loss: 0.5158 - val_accuracy: 0.9712 - val_loss: 0.1621\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 54ms/step - accuracy: 0.9190 - loss: 0.4622 - val_accuracy: 0.9742 - val_loss: 0.1150\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9119 - loss: 0.4014 - val_accuracy: 0.9690 - val_loss: 0.1351\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9214 - loss: 0.3532 - val_accuracy: 0.9721 - val_loss: 0.1068\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 53ms/step - accuracy: 0.9191 - loss: 0.3440 - val_accuracy: 0.9690 - val_loss: 0.1207\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 54ms/step - accuracy: 0.9254 - loss: 0.2986 - val_accuracy: 0.9682 - val_loss: 0.1059\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 54ms/step - accuracy: 0.9235 - loss: 0.2822 - val_accuracy: 0.9725 - val_loss: 0.0949\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 52ms/step - accuracy: 0.9200 - loss: 0.2532 - val_accuracy: 0.9682 - val_loss: 0.1112\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9183 - loss: 0.2570 - val_accuracy: 0.9746 - val_loss: 0.0845\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9178 - loss: 0.2511 - val_accuracy: 0.9729 - val_loss: 0.0808\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9179 - loss: 0.2437 - val_accuracy: 0.9725 - val_loss: 0.0788\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9188 - loss: 0.2373 - val_accuracy: 0.9712 - val_loss: 0.0787\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 54ms/step - accuracy: 0.9221 - loss: 0.2393 - val_accuracy: 0.9725 - val_loss: 0.0806\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9227 - loss: 0.2314 - val_accuracy: 0.9669 - val_loss: 0.0901\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9166 - loss: 0.2515 - val_accuracy: 0.9751 - val_loss: 0.0746\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9166 - loss: 0.2361 - val_accuracy: 0.9678 - val_loss: 0.1032\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 52ms/step - accuracy: 0.9160 - loss: 0.2625 - val_accuracy: 0.9686 - val_loss: 0.0935\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history4.history[\"accuracy\"], label = \"Training\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history4.history[\"loss\"], label = \"Training\")\nplt.plot(history4.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\nTraining vs.¬†Validation Accuracy * The training accuracy steeply increases and plateaus around 92.5%. * The validation accuracy remains consistently higher than the training accuracy, fluctuating around 96%. * This suggests that the model is well-trained and generalizes well on the validation set.\nTraining vs.¬†Validation Loss * The training loss decreases over epochs but still remains higher than the validation loss. * The validation loss is consistently lower than the training loss. * This unusual trend (validation performing better than training) could be due to strong regularization techniques (e.g., dropout, batch normalization) or differences in dataset distributions.\n\ntest_loss, test_acc = model4.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 41ms/step - accuracy: 0.9652 - loss: 0.1055\nTest Accuracy: 96.35%\n\n\n\nval_loss, val_acc = model4.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 43ms/step - accuracy: 0.9636 - loss: 0.1169\nValidation Accuracy: 96.86%\n\n\n\n\n\nIn bold font, describe the validation accuracy of your model during training. Comment on this validation accuracy in comparison to the accuracy you were able to obtain with model1. Comment again on overfitting. Do you observe overfitting in model4?\n\nThe validation accuracy of this model is 96.86%.\nThe validation accuracy of this model at 96.86% is much higher than our first model that had a validation accuracy of 61.78%. This is an increase of around 35%.\nOverfitting occurs when the model performs significantly better on the training data than on the validation data, meaning that it has memorized the training set rather than generalizing well. In this model, it does not show signs of overfitting since the validation accuracy remains consistently high and does not drop after a certain number of epochs, suggesting strong generalization. Since the test accuracy (96.35%) closely matches the validation accuracy, it confirms that the model is performing well on unseen data and is not overfitting."
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#setting-up-project",
    "href": "posts/HW5_ImageClassification/index.html#setting-up-project",
    "title": "Image Classification",
    "section": "",
    "text": "As always, we‚Äôre going to start off with importing all the necessary libraries.\n\nimport os\nimport keras\nfrom keras import datasets, layers, models\nimport tensorflow as tf\nfrom keras import utils\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n\n\n\n\nWe got the following dataset from Kaggle that gives us labeled images of cats and dogs to work with.\nTo work with the dataset from TFDS (TensorFlow Datasets), we can load it and split it into 40% training, 10% validation, and 10% test sets:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% training, 10% validation, and 10% test\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    # Dataset is returned as (image, label) pairs\n    as_supervised=True,\n)\n\n# Check number of samples in each split\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\nThis next block contains code that helps us resize the images (of different shapes and sizes) to a fixed size of 150x150.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\n\n\nAnd finally, we will be optimizing our data for it to better work with our deep learning models, especially when we are working with large datasets.\n.batch(batch_size): groups images in groups of ‚Äúbatch_size‚Äù (64 in this case) which in turn improves memory efficiency and parallel processing\n.prefetch(tf_data.AUTOTUNE): allows the model to ‚Äúfetch‚Äù the next batch of images, all while processing the current batch\n.cache(): if possible, stores dataset in memory\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#working-with-datasets",
    "href": "posts/HW5_ImageClassification/index.html#working-with-datasets",
    "title": "Image Classification",
    "section": "",
    "text": "In this next part, we will be writing a function to get used to working with datasets. In this function, we have to create a two-row visualization where the first row shows images of three random cats and the second row shows images of three random dogs.\nFirst, we‚Äôll have to seperate the dataset into two: images and labels. From there, we will use the label to figure out if it is an image of a dog or a cat. Then, it is easy to seperate and plot the two.\n\ndef show_images(train_ds):\n  # Convert dataset into 2 lists\n  image_list = []\n  label_list = []\n\n  # Iterate through small batch of images/labels\n  for image, label in train_ds.take(1).as_numpy_iterator():\n      image_list.append(image)\n      label_list.append(label)\n\n  # Convert lists into NumPy arrays\n  image_array = np.array(image_list)\n  label_array = np.array(label_list)\n\n  # Split images into dogs and cats\n  cats = image_array[label_array == 0]\n  dogs = image_array[label_array == 1]\n\n  # Pick 3 random cat and dog images\n  num_samples = min(len(cats), len(dogs), 3)\n\n  random_cats = random.sample(list(cats), num_samples)\n  random_dogs = random.sample(list(dogs), num_samples)\n\n  # Display\n  fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n\n  # 3 cat images in the first row\n  for i in range(3):\n      axes[0, i].imshow(random_cats[i] / 255.0)\n      axes[0, i].axis(\"off\")\n      axes[0, i].set_title(\"Cat\")\n\n  # 3 dog images in the second row\n  for i in range(3):\n      axes[1, i].imshow(random_dogs[i] / 255.0)\n      axes[1, i].axis(\"off\")\n      axes[1, i].set_title(\"Dog\")\n\n  # Show\n  plt.tight_layout()\n  plt.show()\n\n\nshow_images(train_ds)\n\n\n\n\n\n\n\n\nAwww‚Ä¶how cute!"
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#label-distribution-and-baseline-accuracy",
    "href": "posts/HW5_ImageClassification/index.html#label-distribution-and-baseline-accuracy",
    "title": "Image Classification",
    "section": "",
    "text": "Before we train our model, we want to analyze our datasets label distribution, which is, how many dogs and cats are there? This line of code extracts the labels from the dataset.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nWith that information, we can print out the number of cats and dogs in our dataset to make sure there‚Äôs not an imbalance between the two.\n\nlabels = np.array(list(labels_iterator))\n\nnum_cats = np.sum(labels == 0)\nnum_dogs = np.sum(labels == 1)\n\nprint(f\"Number of cats: {num_cats}\")\nprint(f\"Number of dogs: {num_dogs}\")\n\nNumber of cats: 4637\nNumber of dogs: 4668\n\n\n\n\n\nA baseline model is the simplest model that makes predictions, without directly learning from the data. The baseline tends to predict the ‚Äúmajority class‚Äù so if there were 60% cats and 40% dogs, it would guess cats for 60% of the pictures.\nSince our model is around a 50/50 split, we would predict the baseline model accuracy to be around 50%!\n\ntotal = num_cats + num_dogs\nbaseline = (max(num_cats, num_dogs) / total) * 100\nprint(f\"Baseline accuracy: {baseline:.2f}%\")\n\nBaseline accuracy: 50.17%\n\n\nWe were right!"
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#building-and-training-a-cnn-model",
    "href": "posts/HW5_ImageClassification/index.html#building-and-training-a-cnn-model",
    "title": "Image Classification",
    "section": "",
    "text": "Now, let‚Äôs get to the good stuff. We will start off by building a simple Keras Sequential model to classify images of cats and dogs. We want to start off with at least:\n\n2 Conv2D layers: detects small/complex (based on filter size) patterns, like edges and textures, in the image using 32 (64, 128, etc.) filters of size 3x3.\n2 MaxPooling2D layers: reduces the image size by taking the most important features from a 2x2 region to make the model more efficient\n1 Flatten layer: converts 2D image data into a 1D array to prepare it for the fully connected layers\n1 Dropout layer: randomly turns off 50% (or however any) of neurons to prevent overfitting and improve generalization\n1 Dense layer: outputs a probability between 0 and 1 which determines whether the image is a cat or a dog\n\nWe will play around with the parameters until we get at least 55% validation accuracy!\n\n\n\nmodel1 = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n\n\nThe compile() function essentially configures how the model learns! * optimizer=‚Äòadam‚Äô: adjusts weights efficiently to minimize errors during training (adam = Adaptive Moment Estimation) * loss=‚Äúbinary_crossentropy:‚Äù measures how well the model distinguishes between the two classes (cats vs.¬†dogs) * metrics=[‚Äòaccuracy‚Äô]: tracks the percentage of correctly classified images\n\nmodel1.compile(optimizer='adam',\n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])\n\n\n\n\nThe fit() function uses the training data to train the model and validates it using the validation data! * train_ds: this is the dataset that is used for training the model, where the model learns patterns * epochs=20: the model will iterate through the training data 20 times to adjust weights and minimize errors * validation_data=validation_ds: evaluates the model‚Äôs performance after each epoch using the validation dataset which helps track how well the model generalizes to new, unseen data\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18s 74ms/step - accuracy: 0.5214 - loss: 103.9403 - val_accuracy: 0.5327 - val_loss: 0.6821\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.5922 - loss: 0.6596 - val_accuracy: 0.5503 - val_loss: 0.7064\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.6649 - loss: 0.5952 - val_accuracy: 0.5456 - val_loss: 0.7385\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.7212 - loss: 0.5119 - val_accuracy: 0.5641 - val_loss: 0.9743\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 33ms/step - accuracy: 0.7778 - loss: 0.4484 - val_accuracy: 0.5838 - val_loss: 0.8604\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8155 - loss: 0.3833 - val_accuracy: 0.5838 - val_loss: 1.0832\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8565 - loss: 0.3123 - val_accuracy: 0.5632 - val_loss: 1.5482\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8870 - loss: 0.2848 - val_accuracy: 0.5701 - val_loss: 1.4691\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.8932 - loss: 0.2519 - val_accuracy: 0.5825 - val_loss: 1.7660\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9152 - loss: 0.2185 - val_accuracy: 0.5851 - val_loss: 1.6861\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 32ms/step - accuracy: 0.9323 - loss: 0.1865 - val_accuracy: 0.5920 - val_loss: 1.5458\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9348 - loss: 0.1822 - val_accuracy: 0.6045 - val_loss: 1.7919\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 32ms/step - accuracy: 0.9409 - loss: 0.1656 - val_accuracy: 0.5972 - val_loss: 1.8326\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9403 - loss: 0.1635 - val_accuracy: 0.5903 - val_loss: 1.9949\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9279 - loss: 0.2091 - val_accuracy: 0.6195 - val_loss: 1.7677\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9546 - loss: 0.1325 - val_accuracy: 0.6182 - val_loss: 1.8156\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9562 - loss: 0.1256 - val_accuracy: 0.6191 - val_loss: 1.9112\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 32ms/step - accuracy: 0.9640 - loss: 0.1083 - val_accuracy: 0.6238 - val_loss: 1.7145\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9725 - loss: 0.0838 - val_accuracy: 0.6230 - val_loss: 2.0231\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9718 - loss: 0.0929 - val_accuracy: 0.6178 - val_loss: 1.8647\n\n\n\n\n\nWe plot training and validation accuracy data to help us see what is going on with the model. In order to analyze correctly, here is what to look out for:\n\nTraining Accuracy Curve: this line shows how well the model performs on the data it was trained on. It usually improves over time as the model learns.\nValidation Accuracy Curve: this line shows how well the model generalizes to unseen data (validation set). Ideally, this should also increase, but it can sometimes behave differently.\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn our ‚ÄúTraining vs.¬†Validation Accuracy‚Äù graph, we can observe that the training accuracy increases and starts to plateau slowly. Our validation accuracy is significantly lower and remains more or less a flat line throughout, with a slight increase.\nWhat This Means: * Training accuracy higher than validation: indicates that the model is learning well on the training data, but is struggling to generalize to the validation set (a sign of overfitting if the gap widens over time). * Validation accuracy higher than training: this can happen in early training, especially if the model is underfitting (not complex enough). It usually indicates the model hasn‚Äôt fully learned the training data yet.\n\n\n\nIn our ‚ÄúTraining vs.¬†Validation Loss‚Äù graph, we can observe that the training loss quickly drops from high to low, forming an L-shape, while the validation loss remains flatter throughout, with a slight increase.\nWhat This Means: * Validation loss higher than training: his can happen in the early stages of training, especially if the model is underfitting (not complex enough or not learning enough from the data). It typically indicates that the model hasn‚Äôt yet learned to capture the patterns in the training data effectively. Over time, as the model improves, the validation loss should ideally decrease, but if it stays high while training loss continues to decrease, it could indicate a generalization issue. * Training loss higher than validation: this indicates that the model isn‚Äôt learning as efficiently from the training data as it should, and it may need more training time or a more complex model to improve performance.\n\n\n\n\nAfter training the model, it‚Äôs important to see how well the model performs on unseen test data. This is done using the evaluate() method:\n\ntest_loss, test_acc = model1.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 77ms/step - accuracy: 0.5952 - loss: 1.8894\nTest Accuracy: 60.40%\n\n\n\nval_loss, val_acc = model1.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 10ms/step - accuracy: 0.6166 - loss: 1.9826\nValidation Accuracy: 61.78%\n\n\n\n\n\nThe validation accuracy of my model stabilized to 60.58% during training.\nAs calculated earlier, the baseline accuracy was coming out to be around 50%. Our model, with its 61.78% accuracy, is doing better than baseline by around 10%, which shows an improvement in generalization.\nYes, since our training accuracy exceeds validation accuracy, we observe overfitting in our model."
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#adding-data-augmentation",
    "href": "posts/HW5_ImageClassification/index.html#adding-data-augmentation",
    "title": "Image Classification",
    "section": "",
    "text": "To get our model to differentiate between images of cats and dogs better, we will be adding data augmentation layers to our model. What data augmentation layers do is create transformed versions of the input images during training to help the model generalize better. These layers apply random modifications to the images such as flipping and rotating to increase data diversity without needing more labeled images.\n\n\nWe will start of with creating a keras.layers.RandomFlip() layer. We can have this flip the image horizontally, vertically, or both, by specifying with mode. For this example, we will set mode = ‚Äúhorizontal‚Äù which will flip the images along the horizontal axis.\n\nflipped = layers.RandomFlip(mode = \"horizontal\")\n\n\nsample_image, _ = next(iter(train_ds.unbatch().take(1)))\n\n\n# Plot original + 4 flipped versions\nplt.figure(figsize=(10, 5))\n\n# Original image\nplt.subplot(1, 5, 1)\nplt.imshow(sample_image.numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nplt.axis(\"off\")\n\n# Apply RandomFlip multiple times\nfor i in range(4):\n    flipped_image = flipped(sample_image)\n    plt.subplot(1, 5, i + 2)\n    plt.imshow(np.array(flipped_image).astype(\"uint8\"))\n    plt.title(f\"Flip {i+1}\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nTransformations like flipping, rotating, and cropping are applied randomly to each image during training. This means that every time the model runs, the augmented versions of images may look different.\n\n\n\nNext, we will make a keras.layers.RandomRotation() layer which will randomly rotate the image during training and helps the model be better when there are variations in object orientation.\n\nrandomroto = layers.RandomRotation(0.2)\n\n\nsample_image, _ = next(iter(train_ds.unbatch().take(1)))  # Take a single image\n\n\n# Plot original + 4 flipped versions\nplt.figure(figsize=(10, 5))\n\n# Original image\nplt.subplot(1, 5, 1)\nplt.imshow(sample_image.numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nplt.axis(\"off\")\n\n# Apply RandomFlip multiple times\nfor i in range(4):\n    flipped_image = randomroto(sample_image)  # Apply RandomFlip\n    plt.subplot(1, 5, i + 2)\n    plt.imshow(np.array(flipped_image).astype(\"uint8\"))\n    plt.title(f\"Flip {i+1}\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNow, we‚Äôll integrate the data augmentation layers into our model we created above to see if these new layers improve the model‚Äôs ability to distinguish between cats and dogs. By adding random flipping and rotation, we expose the model to more variations in image orientation which in turn helps it learn more generalizable features rather than memorizing specific image patterns to prevent overfitting and improve validation accuracy.\n\n\n\nmodel2 = keras.Sequential([\n    # Data augmentation layers\n    flipped,\n    randomroto,\n\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n\n\n# Compile the model\nmodel2.compile(optimizer=\"adam\",\n               loss=\"binary_crossentropy\",\n               metrics=[\"accuracy\"]\n              )\n\n\n\n\n\n# Train the model for 20 epochs\nhistory2 = model2.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds\n                     )\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11s 44ms/step - accuracy: 0.5437 - loss: 27.9833 - val_accuracy: 0.6131 - val_loss: 0.6547\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.5951 - loss: 0.6623 - val_accuracy: 0.6290 - val_loss: 0.6451\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6008 - loss: 0.6656 - val_accuracy: 0.6341 - val_loss: 0.6394\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6237 - loss: 0.6548 - val_accuracy: 0.6599 - val_loss: 0.6256\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6290 - loss: 0.6462 - val_accuracy: 0.6647 - val_loss: 0.6119\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6346 - loss: 0.6378 - val_accuracy: 0.6660 - val_loss: 0.6300\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6141 - loss: 0.6599 - val_accuracy: 0.6582 - val_loss: 0.6340\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.6373 - loss: 0.6422 - val_accuracy: 0.7094 - val_loss: 0.5763\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6510 - loss: 0.6365 - val_accuracy: 0.7158 - val_loss: 0.5737\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.6490 - loss: 0.6312 - val_accuracy: 0.6707 - val_loss: 0.6139\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6389 - loss: 0.6325 - val_accuracy: 0.6999 - val_loss: 0.5846\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6771 - loss: 0.6134 - val_accuracy: 0.6814 - val_loss: 0.6020\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6574 - loss: 0.6226 - val_accuracy: 0.7111 - val_loss: 0.5728\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6913 - loss: 0.5886 - val_accuracy: 0.7137 - val_loss: 0.5677\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.6877 - loss: 0.5840 - val_accuracy: 0.6986 - val_loss: 0.5988\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6931 - loss: 0.5873 - val_accuracy: 0.7390 - val_loss: 0.5598\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 41ms/step - accuracy: 0.6969 - loss: 0.5830 - val_accuracy: 0.7098 - val_loss: 0.5699\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.7116 - loss: 0.5680 - val_accuracy: 0.7472 - val_loss: 0.5383\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.7123 - loss: 0.5629 - val_accuracy: 0.7485 - val_loss: 0.5332\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.7058 - loss: 0.5668 - val_accuracy: 0.7347 - val_loss: 0.5296\n\n\n\n\n\n\nSame principles as above apply here to analyze our plots.\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history2.history[\"accuracy\"], label = \"Training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history2.history[\"loss\"], label = \"Training\")\nplt.plot(history2.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\nTraining vs.¬†Validation Accuracy * Both training and validation accuracy show an upward trend, which indicates that the model is learning over time. * The validation accuracy closely follows the training accuracy, which indicates minimal overfitting and a good generalization to unseen data. * Accuracy reaches around 62%, which is an improvement from the baseline.\nTraining vs.¬†Validation Loss * The training loss starts very high but drops instantly, forming an ‚ÄúL‚Äù shape. * The validation loss remains flat throughout and much lower than the training loss, which is unusual.\nOverall * The model improves in accuracy, showing successful learning. * There is no extreme overfitting, as validation and training accuracy remain close.\n\ntest_loss, test_acc = model2.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 12ms/step - accuracy: 0.7194 - loss: 0.5441\nTest Accuracy: 73.04%\n\n\n\nval_loss, val_acc = model2.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 14ms/step - accuracy: 0.7368 - loss: 0.5196\nValidation Accuracy: 73.47%\n\n\n\n\n\n\nThe validation accuracy of this model is 73.47%.\nThis validation accuracy of 73.47% shows great improvement from the previous model accuracy of 61.78%. There is an increase of around 12%.\nThere isn‚Äôt any severe overfitting, as validation accuracy does not lag far behind training accuracy."
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#data-preprocessing",
    "href": "posts/HW5_ImageClassification/index.html#data-preprocessing",
    "title": "Image Classification",
    "section": "",
    "text": "To make our model better and more efficient, it is good to apply simple transformations to the input data before training. This is called preprocessing.\nIn this case, our image pixels have RGB values ranging from 0 to 255, but many models train more efficiently when these values are normalized between 0 and 1 (or -1 and 1).\nThe following code block shows us how to normalize our values:\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\n\n\nmodel3 = keras.Sequential([\n    # Preprocessor layer\n    preprocessor,\n\n    flipped,\n    randomroto,\n\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n\n\n# Compile the model\nmodel3.compile(optimizer=\"adam\",\n               loss=\"binary_crossentropy\",\n               metrics=[\"accuracy\"]\n              )\n\n\n\n\n\n# Train the model for 20 epochs\nhistory3 = model3.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds\n                     )\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 41ms/step - accuracy: 0.5534 - loss: 0.9582 - val_accuracy: 0.6496 - val_loss: 0.6330\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6345 - loss: 0.6363 - val_accuracy: 0.7167 - val_loss: 0.5682\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.6792 - loss: 0.5959 - val_accuracy: 0.7240 - val_loss: 0.5391\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.6998 - loss: 0.5779 - val_accuracy: 0.7339 - val_loss: 0.5277\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 41ms/step - accuracy: 0.7166 - loss: 0.5581 - val_accuracy: 0.7244 - val_loss: 0.5331\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7241 - loss: 0.5464 - val_accuracy: 0.7567 - val_loss: 0.5009\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7353 - loss: 0.5260 - val_accuracy: 0.7554 - val_loss: 0.4939\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7376 - loss: 0.5330 - val_accuracy: 0.7601 - val_loss: 0.5035\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7470 - loss: 0.5116 - val_accuracy: 0.7635 - val_loss: 0.4777\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7613 - loss: 0.5041 - val_accuracy: 0.7837 - val_loss: 0.4631\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7628 - loss: 0.4971 - val_accuracy: 0.7760 - val_loss: 0.4826\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7608 - loss: 0.4898 - val_accuracy: 0.7825 - val_loss: 0.4571\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7730 - loss: 0.4863 - val_accuracy: 0.7696 - val_loss: 0.4870\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.7760 - loss: 0.4797 - val_accuracy: 0.7846 - val_loss: 0.4788\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7827 - loss: 0.4714 - val_accuracy: 0.7979 - val_loss: 0.4638\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 39ms/step - accuracy: 0.7794 - loss: 0.4621 - val_accuracy: 0.7945 - val_loss: 0.4559\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7782 - loss: 0.4673 - val_accuracy: 0.8014 - val_loss: 0.4578\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 40ms/step - accuracy: 0.7852 - loss: 0.4618 - val_accuracy: 0.7902 - val_loss: 0.4658\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7970 - loss: 0.4571 - val_accuracy: 0.7898 - val_loss: 0.4626\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 40ms/step - accuracy: 0.7964 - loss: 0.4507 - val_accuracy: 0.8018 - val_loss: 0.4494\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history3.history[\"accuracy\"], label = \"Training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history3.history[\"loss\"], label = \"Training\")\nplt.plot(history3.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\nTraining vs.¬†Validation Accuracy\n\nThe validation accuracy and training accuracy both show a steady increase close together, eventually stabilizing close to around 80%.\nThe validation accuracy remains slightly higher than the training accuracy, which means there is good generalization without much overfitting.\nThe model performs well on unseen data, which means it is learning effectively.\n\nTraining vs.¬†Validation Loss\n\nBoth training and validation loss decrease over time, which is a sign of successful learning.\nThe curves are close, meaning the model is not overfitting and maintains a good balance between training and validation performance.\n\n\ntest_loss, test_acc = model3.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 11ms/step - accuracy: 0.7783 - loss: 0.4538\nTest Accuracy: 79.36%\n\n\n\n# Evaluate on validation dataset\nval_loss, val_acc = model3.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 11ms/step - accuracy: 0.7957 - loss: 0.4574\nValidation Accuracy: 80.18%\n\n\n\n\n\nThe validation accuracy of our model with an additional layer of preprocessing reaches 80.18%.\nThe validation accuracy of this model is a significant increase from model 1 from 61.78% accuracy to 80.18% accuracy, an increase of around 19%.\nThere is no significant overfitting observed in this model, as the validation accuracy closely follows the training accuracy, indicating good generalization to unseen data."
  },
  {
    "objectID": "posts/HW5_ImageClassification/index.html#transfer-learning",
    "href": "posts/HW5_ImageClassification/index.html#transfer-learning",
    "title": "Image Classification",
    "section": "",
    "text": "What we have been doing thus far is training models from scratch to distinguish between cats and dogs. But what if we could build on existing knowledge instead of starting fresh? That‚Äôs where transfer learning comes in.\nRather than training a model from the ground up, we can use a pretrained base model, which is a model that has already learned useful features from a massive dataset. For image classification tasks, models like MobileNetV3Large have been trained on millions of images and can recognize patterns like edges, textures, and shapes. By incorporating such a model into our workflow, we can take advantage of these learned features and fine-tune them for our specific task.\nThe following code downloads MobileNetV3Large and configures it as a layer in our model:\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 0us/step\n\n\n\n\n\nmodel4 = keras.Sequential([\n    flipped,\n    randomroto,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\n\n\n\n\nmodel4.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy']\n               )\n\n\n\n\n\nhistory4 = model4.fit(train_ds,\n                      epochs=20,\n                      validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18s 74ms/step - accuracy: 0.6939 - loss: 3.4024 - val_accuracy: 0.9540 - val_loss: 0.2699\nEpoch 2/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 17s 54ms/step - accuracy: 0.8867 - loss: 0.8977 - val_accuracy: 0.9678 - val_loss: 0.1746\nEpoch 3/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 55ms/step - accuracy: 0.9043 - loss: 0.7033 - val_accuracy: 0.9712 - val_loss: 0.1501\nEpoch 4/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9139 - loss: 0.5158 - val_accuracy: 0.9712 - val_loss: 0.1621\nEpoch 5/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 54ms/step - accuracy: 0.9190 - loss: 0.4622 - val_accuracy: 0.9742 - val_loss: 0.1150\nEpoch 6/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9119 - loss: 0.4014 - val_accuracy: 0.9690 - val_loss: 0.1351\nEpoch 7/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9214 - loss: 0.3532 - val_accuracy: 0.9721 - val_loss: 0.1068\nEpoch 8/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 53ms/step - accuracy: 0.9191 - loss: 0.3440 - val_accuracy: 0.9690 - val_loss: 0.1207\nEpoch 9/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 54ms/step - accuracy: 0.9254 - loss: 0.2986 - val_accuracy: 0.9682 - val_loss: 0.1059\nEpoch 10/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 54ms/step - accuracy: 0.9235 - loss: 0.2822 - val_accuracy: 0.9725 - val_loss: 0.0949\nEpoch 11/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 52ms/step - accuracy: 0.9200 - loss: 0.2532 - val_accuracy: 0.9682 - val_loss: 0.1112\nEpoch 12/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9183 - loss: 0.2570 - val_accuracy: 0.9746 - val_loss: 0.0845\nEpoch 13/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9178 - loss: 0.2511 - val_accuracy: 0.9729 - val_loss: 0.0808\nEpoch 14/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9179 - loss: 0.2437 - val_accuracy: 0.9725 - val_loss: 0.0788\nEpoch 15/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9188 - loss: 0.2373 - val_accuracy: 0.9712 - val_loss: 0.0787\nEpoch 16/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 54ms/step - accuracy: 0.9221 - loss: 0.2393 - val_accuracy: 0.9725 - val_loss: 0.0806\nEpoch 17/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9227 - loss: 0.2314 - val_accuracy: 0.9669 - val_loss: 0.0901\nEpoch 18/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9166 - loss: 0.2515 - val_accuracy: 0.9751 - val_loss: 0.0746\nEpoch 19/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 53ms/step - accuracy: 0.9166 - loss: 0.2361 - val_accuracy: 0.9678 - val_loss: 0.1032\nEpoch 20/20\n146/146 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 52ms/step - accuracy: 0.9160 - loss: 0.2625 - val_accuracy: 0.9686 - val_loss: 0.0935\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history4.history[\"accuracy\"], label = \"Training\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history4.history[\"loss\"], label = \"Training\")\nplt.plot(history4.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\nTraining vs.¬†Validation Accuracy * The training accuracy steeply increases and plateaus around 92.5%. * The validation accuracy remains consistently higher than the training accuracy, fluctuating around 96%. * This suggests that the model is well-trained and generalizes well on the validation set.\nTraining vs.¬†Validation Loss * The training loss decreases over epochs but still remains higher than the validation loss. * The validation loss is consistently lower than the training loss. * This unusual trend (validation performing better than training) could be due to strong regularization techniques (e.g., dropout, batch normalization) or differences in dataset distributions.\n\ntest_loss, test_acc = model4.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 41ms/step - accuracy: 0.9652 - loss: 0.1055\nTest Accuracy: 96.35%\n\n\n\nval_loss, val_acc = model4.evaluate(validation_ds)\nprint(f\"Validation Accuracy: {val_acc:.2%}\")\n\n37/37 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 43ms/step - accuracy: 0.9636 - loss: 0.1169\nValidation Accuracy: 96.86%\n\n\n\n\n\nIn bold font, describe the validation accuracy of your model during training. Comment on this validation accuracy in comparison to the accuracy you were able to obtain with model1. Comment again on overfitting. Do you observe overfitting in model4?\n\nThe validation accuracy of this model is 96.86%.\nThe validation accuracy of this model at 96.86% is much higher than our first model that had a validation accuracy of 61.78%. This is an increase of around 35%.\nOverfitting occurs when the model performs significantly better on the training data than on the validation data, meaning that it has memorized the training set rather than generalizing well. In this model, it does not show signs of overfitting since the validation accuracy remains consistently high and does not drop after a certain number of epochs, suggesting strong generalization. Since the test accuracy (96.35%) closely matches the validation accuracy, it confirms that the model is performing well on unseen data and is not overfitting."
  },
  {
    "objectID": "posts/HW0_PenguinVisualization/index.html",
    "href": "posts/HW0_PenguinVisualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Introduction\nWelcome to the first tutorial of the year! In this tutorial, we will be starting off with the basics of data visualization using Matplotlib. We will walk through, step by step, on how to create an interesting and readable data visualization using the Palmer Penguins dataset. We will focus on plotting a scatter plot to explore the relationship between the body mass and flipper length of the penguins.\nFirst, we will load the dataset. Then, we will generate the scatterplot using Python‚Äôs matplotlib library.\n\n\n\nStep 1: Import the Necessary Libraries\nLet‚Äôs begin by importing pandas to handle the dataset and matplotlib.pyplot to create our scatter plot.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nStep 2: Load the Data\nNext, we‚Äôll load the Palmer Penguins dataset from the provided URL and create a pandas dataframe. The dataset includes many different measurements for penguins, like body mass and flipper length, as well as other features like sex, culmen length and depth, and more.\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nStep 3: Create a Scatter Plot\nNow that we have the data loaded, let‚Äôs create a scatter plot to visualize the relationship between Body Mass (g) and Flipper Length (mm)!\n\n# Create a figure and axis object for the plot\nfig, ax = plt.subplots(1)\n\n# Set x and y \n# Extract Body Mass and Flipper Length from the dataset \nx = penguins['Body Mass (g)']\ny = penguins['Flipper Length (mm)']\n\n# Plot a scatter plot of Body Mass vs. Flipper Length\nax.scatter(x, y)\n\n# Set the labels for the x-axis and y-axis\nax.set(title = \"Body Mass vs. Flipper Length\", xlabel = \"Body Mass (g)\", ylabel = \"Flipper Length (mm)\");\n\n\n\n\n\n\n\n\n\n\nConclusion\nWith this simple scatter plot, we can visually explore the relationship between the body mass and flipper length of the penguins. This is a great starting point for analyzing more complex datasets and creating insightful visualizations.\n\n\n\nScatterplot"
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html",
    "href": "posts/HW4_HeatDiffusion/index.html",
    "title": "Heat Diffusion",
    "section": "",
    "text": "Welcome back to the fourth blog post in the Python for Beginners series!\nIn this blog post, I‚Äôm going to walk you through how to simulate heat diffusion in two dimensions using different numerical methods.\nWhy?\nBecause it‚Äôs going to be fun, educational, and super satisfying, watching heat spread in a perfectly symmetric way.\nWe will be going over 4 methods to create heat diffusions in the 2D space.\nLet‚Äôs start off with setting up our code.\n\n# Import Necessary Libraries\nimport time\nimport inspect\nimport numpy as np\nfrom jax import jit\nimport heat_equation\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nfrom matplotlib import pyplot as plt\nfrom heat_equation import get_sparse_A\nfrom heat_equation import advance_time_jax\nfrom heat_equation import advance_time_numpy\nfrom heat_equation import get_A, advance_time_matvecmul\n\n\n# Define parameters\nN = 101\nepsilon = 0.2\niterations = 2700\n\nTo avoid repeating code across different heat simulation methods, we will create a function that handles the initialization of the heat source, runs the simulation, and generates visualizations.\nThis function works with all methods throughout this blog, which makes it easy to swap between different approaches without duplicating logic.\n\ndef heat_simulation(N, epsilon, iterations, method, matrix, advance):\n    # Define parameters\n    iterations = iterations\n    \n    # Initialize heat source at center\n    # Checks for what model\n    if advance == advance_time_jax:\n        u = jnp.zeros((N, N))\n        u = u.at[N//2, N//2].set(1.0)\n    else:\n        u = np.zeros((N, N))\n        u[N//2, N//2] = 1.0 \n    \n    # Get matrix A\n    A = matrix(N)\n    \n    # Run a few warm-up iterations to compile (only need for JAX)\n    if advance == advance_time_jax:\n        for _ in range(5):\n            u = advance_time_jax(u, epsilon)\n    \n    # Store for visualization\n    graph = []\n    \n    # Track computation time\n    start_time = time.time()\n    \n    # Run\n    for i in range(iterations):\n        # Checks for which model\n        if advance == advance_time_matvecmul:\n            u = advance(A, u, epsilon)\n        else:\n            u = advance(u, epsilon)\n        \n        # Save every 300 steps\n        if (i+1) % 300 == 0:  \n            graph.append(u.copy())\n    \n    # Track end time\n    end_time = time.time()\n    \n    # Print computation time\n    print(f\"Total computation time ({method}): {end_time - start_time:.2f} seconds\")\n    \n    # Plot results in 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    \n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(graph):\n            im = ax.imshow(graph[idx], cmap='viridis', origin='lower')\n            ax.set_title(f\"Iteration {(idx+1) * 300}\")\n    \n    plt.tight_layout()\n    plt.show()\n\nLet‚Äôs start off with our first (and excruciatingly slow) method."
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html#introduction",
    "href": "posts/HW4_HeatDiffusion/index.html#introduction",
    "title": "Heat Diffusion",
    "section": "",
    "text": "Welcome back to the fourth blog post in the Python for Beginners series!\nIn this blog post, I‚Äôm going to walk you through how to simulate heat diffusion in two dimensions using different numerical methods.\nWhy?\nBecause it‚Äôs going to be fun, educational, and super satisfying, watching heat spread in a perfectly symmetric way.\nWe will be going over 4 methods to create heat diffusions in the 2D space.\nLet‚Äôs start off with setting up our code.\n\n# Import Necessary Libraries\nimport time\nimport inspect\nimport numpy as np\nfrom jax import jit\nimport heat_equation\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nfrom matplotlib import pyplot as plt\nfrom heat_equation import get_sparse_A\nfrom heat_equation import advance_time_jax\nfrom heat_equation import advance_time_numpy\nfrom heat_equation import get_A, advance_time_matvecmul\n\n\n# Define parameters\nN = 101\nepsilon = 0.2\niterations = 2700\n\nTo avoid repeating code across different heat simulation methods, we will create a function that handles the initialization of the heat source, runs the simulation, and generates visualizations.\nThis function works with all methods throughout this blog, which makes it easy to swap between different approaches without duplicating logic.\n\ndef heat_simulation(N, epsilon, iterations, method, matrix, advance):\n    # Define parameters\n    iterations = iterations\n    \n    # Initialize heat source at center\n    # Checks for what model\n    if advance == advance_time_jax:\n        u = jnp.zeros((N, N))\n        u = u.at[N//2, N//2].set(1.0)\n    else:\n        u = np.zeros((N, N))\n        u[N//2, N//2] = 1.0 \n    \n    # Get matrix A\n    A = matrix(N)\n    \n    # Run a few warm-up iterations to compile (only need for JAX)\n    if advance == advance_time_jax:\n        for _ in range(5):\n            u = advance_time_jax(u, epsilon)\n    \n    # Store for visualization\n    graph = []\n    \n    # Track computation time\n    start_time = time.time()\n    \n    # Run\n    for i in range(iterations):\n        # Checks for which model\n        if advance == advance_time_matvecmul:\n            u = advance(A, u, epsilon)\n        else:\n            u = advance(u, epsilon)\n        \n        # Save every 300 steps\n        if (i+1) % 300 == 0:  \n            graph.append(u.copy())\n    \n    # Track end time\n    end_time = time.time()\n    \n    # Print computation time\n    print(f\"Total computation time ({method}): {end_time - start_time:.2f} seconds\")\n    \n    # Plot results in 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    \n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(graph):\n            im = ax.imshow(graph[idx], cmap='viridis', origin='lower')\n            ax.set_title(f\"Iteration {(idx+1) * 300}\")\n    \n    plt.tight_layout()\n    plt.show()\n\nLet‚Äôs start off with our first (and excruciatingly slow) method."
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html#method-1-the-matrix-multiplication-approach",
    "href": "posts/HW4_HeatDiffusion/index.html#method-1-the-matrix-multiplication-approach",
    "title": "Heat Diffusion",
    "section": "Method 1: The Matrix Multiplication Approach",
    "text": "Method 1: The Matrix Multiplication Approach\nIn heat diffusion simulations, matrix multiplication is a powerful tool to model how heat spreads across a grid. We break the 2D grid into smaller points, with each point representing the temperature at that location.\nThe finite difference matrix A, captures how each point is influenced by its neighbors and by multiplying this matrix with the current temperature values, we can quickly calculate the temperature at each point for the next time step. This process allows us to simulate heat diffusion efficiently, using matrix operations to update the entire grid in one go, making sure the simulation runs smoothly and accurately.\nThe advance_time_matvecmul function helps to do just that. It updates the state of by performing a matrix multiplication. By incorporating the finite difference matrix A, it models the heat diffusion process and advances the system one step forward in time, returning the updated temperature grid.\n\n@jit\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, using matrix-vector multiplication\n    \n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    \n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\nTo simulate heat diffusion in a 2D space using matrix-vector multiplication, we first need to define a function get_A(N) in the file heat_equation.py along with the function advance_time_matvecmul. This function takes an integer N as input and returns the corresponding matrix A. This matrix is crucial in representing the heat diffusion over time.\n\ndef get_A(N):\n    \"\"\"\n    Constructs the 2D finite difference matrix A for the heat equation.\n    \n    Args:\n        N (int): Grid size (NxN).\n        \n    Returns:\n        A (numpy.ndarray): Finite difference matrix of size (N^2 x N^2).\n    \"\"\"\n    # Total number of points in the grid\n    n = N * N  \n    diagonals = [\n        -4 * np.ones(n),\n        np.ones(n - 1),\n        np.ones(n - 1),\n        np.ones(n - N),\n        np.ones(n - N)\n    ]\n    \n    # Set zero at the right boundary to avoid wrap-around\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    \n    # Construct the matrix\n    A = (\n        np.diag(diagonals[0]) + \n        np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) +  \n        np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    )\n    \n    return A\n\nNow that we have our matrix A defined, we can run the simulation by calling the get_A(N) function alongside the advance_time_matvecmul() method. This will simulate the heat diffusion process for 2700 iterations, with each iteration representing a time step in the diffusion process.\n\nprint(inspect.getsource(heat_simulation))\n\ndef heat_simulation(N, epsilon, iterations, method, matrix, advance):\n    # Define parameters\n    iterations = iterations\n    \n    # Initialize heat source at center\n    # Checks for what model\n    if advance == advance_time_jax:\n        u = jnp.zeros((N, N))\n        u = u.at[N//2, N//2].set(1.0)\n    else:\n        u = np.zeros((N, N))\n        u[N//2, N//2] = 1.0 \n    \n    # Get matrix A\n    A = matrix(N)\n    \n    # Run a few warm-up iterations to compile (only need for JAX)\n    if advance == advance_time_jax:\n        for _ in range(5):\n            u = advance_time_jax(u, epsilon)\n    \n    # Store for visualization\n    graph = []\n    \n    # Track computation time\n    start_time = time.time()\n    \n    # Run\n    for i in range(iterations):\n        # Checks for which model\n        if advance == advance_time_matvecmul:\n            u = advance(A, u, epsilon)\n        else:\n            u = advance(u, epsilon)\n        \n        # Save every 300 steps\n        if (i+1) % 300 == 0:  \n            graph.append(u.copy())\n    \n    # Track end time\n    end_time = time.time()\n    \n    # Print computation time\n    print(f\"Total computation time ({method}): {end_time - start_time:.2f} seconds\")\n    \n    # Plot results in 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    \n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(graph):\n            im = ax.imshow(graph[idx], cmap='viridis', origin='lower')\n            ax.set_title(f\"Iteration {(idx+1) * 300}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nheat_simulation(N, epsilon, iterations, \"matrix multiplication\", matrix=get_A, advance=advance_time_matvecmul)\n\nTotal computation time (matrix multiplication): 424.18 seconds"
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html#method-2-sparse-matrix-in-jax",
    "href": "posts/HW4_HeatDiffusion/index.html#method-2-sparse-matrix-in-jax",
    "title": "Heat Diffusion",
    "section": "Method 2: Sparse Matrix in JAX",
    "text": "Method 2: Sparse Matrix in JAX\nIn this method, we will be using sparse matrix operations to optimize performance. The matrix A is sparse because most of its elements are zero, representing the fact that each grid point only interacts with its immediate neighbors. Instead of storing the entire matrix, we use a Batched Coordinate (BCOO) sparse format, which stores only the nonzero values. This reduces both the space and computational complexity to O(N^2), making it faster.\nHere‚Äôs the implementation of get_sparse_A(N) to return A in sparse format:\n\ndef get_sparse_A(N):\n    \"\"\"\n    Returns the finite difference matrix A in a sparse format compatible \n    with JAX.\n    \n    Args:\n        N (int): The size of the grid (NxN).\n        \n    Returns:\n        A_sp_matrix: The sparse finite difference matrix A in BCOO format,  \n                     which is efficient for sparse matrix operations in JAX.\n    \"\"\"\n    dense_matrix = jnp.array(get_A(N))\n\n    # Convert to sparse BCOO format\n    A_sp_matrix = sparse.BCOO.fromdense(dense_matrix)\n    \n    return A_sp_matrix\n\nNow that we have our matrix A defined, we can run the simulation by calling the get_sparse_A(N) function alongside the jit-ed version of advance_time_matvecmul() method. This will simulate the heat diffusion process for 2700 iterations, with each iteration representing a time step in the diffusion process.\n\nprint(inspect.getsource(heat_simulation))\n\ndef heat_simulation(N, epsilon, iterations, method, matrix, advance):\n    # Define parameters\n    iterations = iterations\n    \n    # Initialize heat source at center\n    # Checks for what model\n    if advance == advance_time_jax:\n        u = jnp.zeros((N, N))\n        u = u.at[N//2, N//2].set(1.0)\n    else:\n        u = np.zeros((N, N))\n        u[N//2, N//2] = 1.0 \n    \n    # Get matrix A\n    A = matrix(N)\n    \n    # Run a few warm-up iterations to compile (only need for JAX)\n    if advance == advance_time_jax:\n        for _ in range(5):\n            u = advance_time_jax(u, epsilon)\n    \n    # Store for visualization\n    graph = []\n    \n    # Track computation time\n    start_time = time.time()\n    \n    # Run\n    for i in range(iterations):\n        # Checks for which model\n        if advance == advance_time_matvecmul:\n            u = advance(A, u, epsilon)\n        else:\n            u = advance(u, epsilon)\n        \n        # Save every 300 steps\n        if (i+1) % 300 == 0:  \n            graph.append(u.copy())\n    \n    # Track end time\n    end_time = time.time()\n    \n    # Print computation time\n    print(f\"Total computation time ({method}): {end_time - start_time:.2f} seconds\")\n    \n    # Plot results in 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    \n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(graph):\n            im = ax.imshow(graph[idx], cmap='viridis', origin='lower')\n            ax.set_title(f\"Iteration {(idx+1) * 300}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nheat_simulation(N, epsilon, iterations, \"sparse matrix in JAX\", matrix=get_sparse_A, advance=advance_time_matvecmul)\n\nTotal computation time (sparse matrix in JAX): 1.29 seconds"
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html#method-3-direct-operation-with-numpy",
    "href": "posts/HW4_HeatDiffusion/index.html#method-3-direct-operation-with-numpy",
    "title": "Heat Diffusion",
    "section": "Method 3: Direct Operation with NumPy",
    "text": "Method 3: Direct Operation with NumPy\nWe can simplify the solution by using vectorized array operations, which makes it more efficient to shifting grid elements and simulate heat diffusion.\nHere‚Äôs how you can implement the advance_time_numpy() function in heat_equation.py to advance the solution by one timestep:\nThis function advances the state of the grid by one timestep, simulating how heat diffuses across a 2D grid of size N√óN, where each grid point represents a temperature. The function uses the finite difference method to approximate the heat equation, specifically its Laplacian operator, which describes how the temperature changes based on neighboring points.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of heat diffusion by one timestep using \n    explicit indexing and zero-padding.\n    \n    Args:\n        u (numpy.ndarray): N x N grid state at timestep k.\n        epsilon (float): Stability constant.\n    \n    Returns:\n        numpy.ndarray: N x N Grid state at timestep k+1.\n    \"\"\"\n    # Pad the input array with zeros (1 layer around the grid)\n    padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Compute the Laplacian\n    laplacian = (\n        # Shift down\n        padded[2:, 1:-1] +  \n        # Shift up\n        padded[0:-2, 1:-1] +  \n        # Shift right\n        padded[1:-1, 2:] +  \n        # Shift left\n        padded[1:-1, 0:-2] -\n        # Center\n        4 * u  \n    )\n\n    # Update\n    u_new = u + epsilon * laplacian\n\n    return u_new\n\nThe advance_time_numpy() function simulates heat diffusion in a grid. It first pads the input grid with zeros to handle boundary conditions, then computes the Laplacian by explicitly indexing the padded grid to access neighboring points (up, down, left, right).\nThis updated grid is returned as the new state at the next timestep. The method is efficient and uses NumPy‚Äôs slicing and padding capabilities to avoid boundary issues.\nNow that we have our advance_time_numpy() function, we can run the simulation for 2700 iterations, just like before. We will also store snapshots of the grid every 300 iterations to visualize how the heat diffuses over time.\nHere‚Äôs how you can implement the simulation:\n\nprint(inspect.getsource(heat_simulation))\n\ndef heat_simulation(N, epsilon, iterations, method, matrix, advance):\n    # Define parameters\n    iterations = iterations\n    \n    # Initialize heat source at center\n    # Checks for what model\n    if advance == advance_time_jax:\n        u = jnp.zeros((N, N))\n        u = u.at[N//2, N//2].set(1.0)\n    else:\n        u = np.zeros((N, N))\n        u[N//2, N//2] = 1.0 \n    \n    # Get matrix A\n    A = matrix(N)\n    \n    # Run a few warm-up iterations to compile (only need for JAX)\n    if advance == advance_time_jax:\n        for _ in range(5):\n            u = advance_time_jax(u, epsilon)\n    \n    # Store for visualization\n    graph = []\n    \n    # Track computation time\n    start_time = time.time()\n    \n    # Run\n    for i in range(iterations):\n        # Checks for which model\n        if advance == advance_time_matvecmul:\n            u = advance(A, u, epsilon)\n        else:\n            u = advance(u, epsilon)\n        \n        # Save every 300 steps\n        if (i+1) % 300 == 0:  \n            graph.append(u.copy())\n    \n    # Track end time\n    end_time = time.time()\n    \n    # Print computation time\n    print(f\"Total computation time ({method}): {end_time - start_time:.2f} seconds\")\n    \n    # Plot results in 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    \n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(graph):\n            im = ax.imshow(graph[idx], cmap='viridis', origin='lower')\n            ax.set_title(f\"Iteration {(idx+1) * 300}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nheat_simulation(N, epsilon, iterations, \"NumPy\", matrix=get_A, advance=advance_time_numpy)\n\nTotal computation time (NumPy): 0.21 seconds"
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html#method-4-with-jax",
    "href": "posts/HW4_HeatDiffusion/index.html#method-4-with-jax",
    "title": "Heat Diffusion",
    "section": "Method 4: With Jax",
    "text": "Method 4: With Jax\nIn this method of the heat diffusion simulation, we take advantage of JAX‚Äôs Just-In-Time (JIT) compilation to speed up the computation. By using jax.numpy and the @jit decorator, we optimize the advance_time_jax() function for efficient execution.\nThe function simulates heat diffusion by updating the grid state using explicit slicing, similar to the advance_time_numpy() function, but with JAX‚Äôs array operations. Here is a way to implement it:\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the heat equation using jax.numpy with explicit slicing.\n    \n    Args:\n        u (jax.numpy.ndarray): N x N grid state at timestep k.\n        epsilon (float): Stability constant.\n    \n    Returns:\n        jax.numpy.ndarray: N x N Grid state at timestep k+1.\n    \"\"\"\n    # Pad the input array with zeros\n    padded = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Compute the Laplacian using slicing\n    laplacian = (\n        padded[2:, 1:-1] +  # Shift down\n        padded[0:-2, 1:-1] +  # Shift up\n        padded[1:-1, 2:] +  # Shift right\n        padded[1:-1, 0:-2] -  # Shift left\n        4 * u  # Center\n    )\n\n    # Update with forward Euler step\n    u_new = u + epsilon * laplacian\n\n    return u_new\n\nNow that we have our advance_time_jax() function, we can run the simulation for 2700 iterations, just like before. We will also store snapshots of the grid every 300 iterations to visualize how the heat diffuses over time.\nHere‚Äôs how you can implement the simulation:\n\nprint(inspect.getsource(heat_simulation))\n\ndef heat_simulation(N, epsilon, iterations, method, matrix, advance):\n    # Define parameters\n    iterations = iterations\n    \n    # Initialize heat source at center\n    # Checks for what model\n    if advance == advance_time_jax:\n        u = jnp.zeros((N, N))\n        u = u.at[N//2, N//2].set(1.0)\n    else:\n        u = np.zeros((N, N))\n        u[N//2, N//2] = 1.0 \n    \n    # Get matrix A\n    A = matrix(N)\n    \n    # Run a few warm-up iterations to compile (only need for JAX)\n    if advance == advance_time_jax:\n        for _ in range(5):\n            u = advance_time_jax(u, epsilon)\n    \n    # Store for visualization\n    graph = []\n    \n    # Track computation time\n    start_time = time.time()\n    \n    # Run\n    for i in range(iterations):\n        # Checks for which model\n        if advance == advance_time_matvecmul:\n            u = advance(A, u, epsilon)\n        else:\n            u = advance(u, epsilon)\n        \n        # Save every 300 steps\n        if (i+1) % 300 == 0:  \n            graph.append(u.copy())\n    \n    # Track end time\n    end_time = time.time()\n    \n    # Print computation time\n    print(f\"Total computation time ({method}): {end_time - start_time:.2f} seconds\")\n    \n    # Plot results in 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    \n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(graph):\n            im = ax.imshow(graph[idx], cmap='viridis', origin='lower')\n            ax.set_title(f\"Iteration {(idx+1) * 300}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nheat_simulation(N, epsilon, iterations, \"JAX\", matrix=get_A, advance=advance_time_jax)\n\nTotal computation time (JAX): 0.10 seconds\n\n\n\n\n\n\n\n\n\nTo make sure that the code runs optimally, we first run a few warm-up iterations, allowing JAX to compile the function. Then, we run the full simulation for 2700 iterations. The performance boost comes from JAX‚Äôs ability to compile the function ahead of time and execute it much faster than pure Python or NumPy-based implementations. By visualizing the heat diffusion every 300 iterations, we can track the progress of the simulation, ensuring everything works as expected.\nThe JAX approach is not only more efficient in terms of performance but also avoids the need for explicit matrix multiplication, making the code simpler while still achieving high-performance results."
  },
  {
    "objectID": "posts/HW4_HeatDiffusion/index.html#comparison",
    "href": "posts/HW4_HeatDiffusion/index.html#comparison",
    "title": "Heat Diffusion",
    "section": "Comparison",
    "text": "Comparison\n1. Matrix-Vector Multiplication: - Performance: Slow due to inefficiency - 424.18 seconds - Ease: Hard to understand mathematically and complex to implement.\n2. Sparse Matrix Representation: - Performance: Faster than dense matrices by focusing on non-zero elements - 1.29 seconds - Ease: Easier to implement due to sparse matrices.\n3. NumPy-based Computation: - Performance: Fast and efficient - 0.21 seconds - Ease: Easy to implement using simple array slicing and comfort with NumPy.\n4. JAX-based Computation with JIT: - Performance: The fastest due to JIT compilation - 0.10 seconds - Ease: Similar to NumPy.\nConclusion: - Fastest: JAX with JIT. - Easiest to Write: NumPy-based because I am most comfortable with NumPy."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Anahita‚Äôs Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HW2_Scrapy/index.html",
    "href": "posts/HW2_Scrapy/index.html",
    "title": "Lights, Camera, Scrapy!",
    "section": "",
    "text": "Welcome back to the third blog post in the Python for Beginners series!\nIn this blog post, I‚Äôm going to walk you through how I built a web scraper using Scrapy to extract and analyze movie cast data from The Movie Database (TMDB).\n\n\n\n\nStart off with picking your favorite movie and find its TMDB page by searching on https://www.themoviedb.org/. For this tutorial, we will be choosing: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/ - Save this URL for now.\n\n\n\nBefore jumping into coding, it‚Äôs important to understand the navigation steps our scraper will take. This helps us to make sure that we are targeting the right elements and URLs. Manually click through the site and try to find a structure of the web pages we need to scrape.\nSteps:\n\nStart on a movie‚Äôs main page and locate the Full Cast & Crew link. Click it.\nObserve that the URL now follows the pattern /cast/.\nScroll to the Cast section and click on an actor‚Äôs profile picture.\nThis leads to an actor-specific page with a URL like https://www.themoviedb.org/person/4566-alan-rickman.\nScroll down to the Acting section and take note of movie and TV show titles listed.\n\n\n\n\nIn your terminal, write the following lines to initialize your project:\n\nconda activate PIC16B-25W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\nBefore running our scraper, we need to make a small tweak in the Scrapy settings to prevent excessive data collection during testing. Open settings.py and add the following line:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis will make sure that our scraper stops after processing 20 pages, allowing us to check that everything is working correctly without being overwhelming to our system. Once we‚Äôre okay with the scraper‚Äôs behavior, we can remove this limit.\n\n\n\nAs we start making automated requests, TMDB may detect that we are using a scraper and block our requests, resulting in a 403 Forbidden error. This happens when the website identifies that a bot (rather than a human) is making the requests. A simple workaround is modifying the User-Agent in settings.py to mimic a real browser.\nRun the following command in Terminal:\n\nscrapy shell -s USER_AGENT='Scrapy/2.8.0 (+https://scrapy.org)' https://www.themoviedb.org/...\n\n\n\n\n\nOur first step is creating a new Python file: inside the tmdb_scraper/spiders/ directory, create a new Python file called tmdb_spider.py to implement our scraper.\nOur scraper will work in three main steps which we will implement in functions:\n\nFirst, it will parse the main movie page which will extract the URL for the full cast page.\nThen, it will parse the full credits page which will extract actor links.\nAnd finally, it will parse actor pages which will then extract the actor‚Äôs name and the list of movies they have acted in.\n\n\n\nThe scraper starts by navigating to a movie‚Äôs main page. From here, it constructs a URL to the full cast and crew page and sends a request to parse it.\n\ndef parse(self, response):\n        \"\"\"\n        Parses the movie page and navigates to the Full Cast & Crew page.\n\n        Yields:\n            scrapy.Request: A request to the Full Cast & Crew page, calling 'parse_full_credits'.\n        \"\"\"\n        # Hardcoding the path to the Full Cast & Crew page\n        full_cast_url = response.url + \"/cast\"\n    \n        # Yield a request to the full credits page, calling parse_full_credits\n        yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n\n\n\n\nOnce on the full cast and crew page, we extract all actor profile links (ignoring crew members) and send requests to each actor‚Äôs page.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Extracts actor links from the Full Cast & Crew page and navigates to each actor's page.\n\n        Args:\n            response: The HTTP response for the Full Cast & Crew page.\n        \n        Yields:\n            scrapy.Request: Requests for each actor's page, using `parse_actor_page` as the callback.\n        \"\"\"\n        # Select actor links while excluding crew members\n        actor_links = response.xpath(\"//ol[contains(@class, 'people credits ')]/li[not(contains(@class, 'people credits crew')) and not(ancestor::div[contains(@class, 'crew_wrapper')])]/a[@href]/@href\").getall()\n        \n        # Yield requests for actor pages, using `parse_actor_page` as the callback\n        for link in actor_links:\n            full_url = response.urljoin(link)  \n            yield scrapy.Request(url=full_url, callback=self.parse_actor_page)\n\n\n\n\nOn the actor‚Äôs page, we extract their name and list of movies/TV shows they‚Äôve appeared in.\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Extracts actor information and acting credits from the actor's page.\n\n        Args:\n            response: The HTTP response for the actor's page.\n\n        Yields:\n            dict: A dictionary containing the actor's name and a unique movie/TV name.\n        \"\"\"\n        # Extract actor's name from the page header\n        actor_name = response.css('h2.title a::text').get()\n        if actor_name:\n            actor_name = actor_name.strip()\n        else:\n            return\n\n        # Find the Acting section and extract movie/TV show titles\n        acting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\n        if not acting_section:\n            return\n        movie_titles = acting_section.xpath(\".//bdi/text()\").getall()\n\n        # Remove duplicate titles using a set\n        unique_titles = set(movie_titles)\n\n        # Yield a dictionary for each unique movie/TV show\n        for title in unique_titles:\n            yield {\n                \"actor\": actor_name,\n                \"movie_or_TV_name\": title.strip()\n            }\n\n\n\n\n\nTo run the scraper, use the following command in the Terminal:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nFeel free to use a ‚Äúsubdir‚Äù of your choice! This is just an example using Harry Potter and the Philosopher‚Äôs Stone.\nThis will scrape the cast data for Harry Potter and the Philosopher‚Äôs Stone and save the results to results.csv.\n\n\n\nAfter running the recommendation system, I decided to create a simple but intuitive dataframe to find which movies and TV shows share the most actors with our target film.\nImport libraries:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt  \nfrom collections import defaultdict\n\nCreate a data frame that has two columns: Movie/TV Name and Number of Actors Shared:\n\n# Convert the scraped data in the CSV file to a data frame\nresults = pd.read_csv(\"results.csv\")\n\n# Create a dictionary to count how many times each movie/TV show appears\n# Key: Movie/TV show name, Value: Number of shared actors\nmovie_map = defaultdict(int)\n\n# Iterate over each row in the results DataFrame\nfor index, row in results.iterrows():\n    # Skip the target movie\n    if row.at[\"movie_or_TV_name\"] == \"Harry Potter and the Philosopher's Stone\":\n        continue\n    # Increment the count for the current movie/TV show\n    movie_map[row.at[\"movie_or_TV_name\"]]+=1\n\n# Convert the dictionary into a data frame\nshared_actors = pd.DataFrame(list(movie_map.items()), columns=['Movie/TV Name', 'Number of Shared Actors'])\n# Sort the data frame\nshared_actors = shared_actors.sort_values(by='Number of Shared Actors', ascending=False)\n\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie/TV Name\nNumber of Shared Actors\n\n\n\n\n102\nHarry Potter and the Chamber of Secrets\n25\n\n\n37\nHarry Potter and the Prisoner of Azkaban\n15\n\n\n72\nHarry Potter and the Deathly Hallows: Part 2\n15\n\n\n173\nDoctor Who\n12\n\n\n53\nHarry Potter and the Deathly Hallows: Part 1\n12\n\n\n...\n...\n...\n\n\n564\nMo\n1\n\n\n565\nStepping Out\n1\n\n\n566\nArtsnight\n1\n\n\n567\nBathtime\n1\n\n\n1234\n1Life\n1\n\n\n\n\n1235 rows √ó 2 columns\n\n\n\nUsing this data frame, create an intuitive visualization using Matplotlib\n\n# Create bar chart\nplt.figure(figsize=(12, 6))\nplt.barh(shared_actors[\"Movie/TV Name\"].head(10), shared_actors[\"Number of Shared Actors\"].head(10), color=\"lightblue\")  \nplt.xlabel(\"Number of Shared Actors\")  \nplt.ylabel(\"Movie / TV Show\")  \nplt.title(\"Top Movies & TV Shows Sharing Actors with Target Movie\")  \nplt.gca().invert_yaxis()  \nplt.show()  \n\n\n\n\n\n\n\n\n\n\n\nAnd that‚Äôs it for this week‚Äôs blog post! I hope you learned something about the basics of web scraping and hope to see you next week!"
  },
  {
    "objectID": "posts/HW2_Scrapy/index.html#introduction",
    "href": "posts/HW2_Scrapy/index.html#introduction",
    "title": "Lights, Camera, Scrapy!",
    "section": "",
    "text": "Welcome back to the third blog post in the Python for Beginners series!\nIn this blog post, I‚Äôm going to walk you through how I built a web scraper using Scrapy to extract and analyze movie cast data from The Movie Database (TMDB).\n\n\n\n\nStart off with picking your favorite movie and find its TMDB page by searching on https://www.themoviedb.org/. For this tutorial, we will be choosing: https://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/ - Save this URL for now.\n\n\n\nBefore jumping into coding, it‚Äôs important to understand the navigation steps our scraper will take. This helps us to make sure that we are targeting the right elements and URLs. Manually click through the site and try to find a structure of the web pages we need to scrape.\nSteps:\n\nStart on a movie‚Äôs main page and locate the Full Cast & Crew link. Click it.\nObserve that the URL now follows the pattern /cast/.\nScroll to the Cast section and click on an actor‚Äôs profile picture.\nThis leads to an actor-specific page with a URL like https://www.themoviedb.org/person/4566-alan-rickman.\nScroll down to the Acting section and take note of movie and TV show titles listed.\n\n\n\n\nIn your terminal, write the following lines to initialize your project:\n\nconda activate PIC16B-25W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\nBefore running our scraper, we need to make a small tweak in the Scrapy settings to prevent excessive data collection during testing. Open settings.py and add the following line:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis will make sure that our scraper stops after processing 20 pages, allowing us to check that everything is working correctly without being overwhelming to our system. Once we‚Äôre okay with the scraper‚Äôs behavior, we can remove this limit.\n\n\n\nAs we start making automated requests, TMDB may detect that we are using a scraper and block our requests, resulting in a 403 Forbidden error. This happens when the website identifies that a bot (rather than a human) is making the requests. A simple workaround is modifying the User-Agent in settings.py to mimic a real browser.\nRun the following command in Terminal:\n\nscrapy shell -s USER_AGENT='Scrapy/2.8.0 (+https://scrapy.org)' https://www.themoviedb.org/...\n\n\n\n\n\nOur first step is creating a new Python file: inside the tmdb_scraper/spiders/ directory, create a new Python file called tmdb_spider.py to implement our scraper.\nOur scraper will work in three main steps which we will implement in functions:\n\nFirst, it will parse the main movie page which will extract the URL for the full cast page.\nThen, it will parse the full credits page which will extract actor links.\nAnd finally, it will parse actor pages which will then extract the actor‚Äôs name and the list of movies they have acted in.\n\n\n\nThe scraper starts by navigating to a movie‚Äôs main page. From here, it constructs a URL to the full cast and crew page and sends a request to parse it.\n\ndef parse(self, response):\n        \"\"\"\n        Parses the movie page and navigates to the Full Cast & Crew page.\n\n        Yields:\n            scrapy.Request: A request to the Full Cast & Crew page, calling 'parse_full_credits'.\n        \"\"\"\n        # Hardcoding the path to the Full Cast & Crew page\n        full_cast_url = response.url + \"/cast\"\n    \n        # Yield a request to the full credits page, calling parse_full_credits\n        yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n\n\n\n\nOnce on the full cast and crew page, we extract all actor profile links (ignoring crew members) and send requests to each actor‚Äôs page.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Extracts actor links from the Full Cast & Crew page and navigates to each actor's page.\n\n        Args:\n            response: The HTTP response for the Full Cast & Crew page.\n        \n        Yields:\n            scrapy.Request: Requests for each actor's page, using `parse_actor_page` as the callback.\n        \"\"\"\n        # Select actor links while excluding crew members\n        actor_links = response.xpath(\"//ol[contains(@class, 'people credits ')]/li[not(contains(@class, 'people credits crew')) and not(ancestor::div[contains(@class, 'crew_wrapper')])]/a[@href]/@href\").getall()\n        \n        # Yield requests for actor pages, using `parse_actor_page` as the callback\n        for link in actor_links:\n            full_url = response.urljoin(link)  \n            yield scrapy.Request(url=full_url, callback=self.parse_actor_page)\n\n\n\n\nOn the actor‚Äôs page, we extract their name and list of movies/TV shows they‚Äôve appeared in.\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Extracts actor information and acting credits from the actor's page.\n\n        Args:\n            response: The HTTP response for the actor's page.\n\n        Yields:\n            dict: A dictionary containing the actor's name and a unique movie/TV name.\n        \"\"\"\n        # Extract actor's name from the page header\n        actor_name = response.css('h2.title a::text').get()\n        if actor_name:\n            actor_name = actor_name.strip()\n        else:\n            return\n\n        # Find the Acting section and extract movie/TV show titles\n        acting_section = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[1]\")\n        if not acting_section:\n            return\n        movie_titles = acting_section.xpath(\".//bdi/text()\").getall()\n\n        # Remove duplicate titles using a set\n        unique_titles = set(movie_titles)\n\n        # Yield a dictionary for each unique movie/TV show\n        for title in unique_titles:\n            yield {\n                \"actor\": actor_name,\n                \"movie_or_TV_name\": title.strip()\n            }\n\n\n\n\n\nTo run the scraper, use the following command in the Terminal:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nFeel free to use a ‚Äúsubdir‚Äù of your choice! This is just an example using Harry Potter and the Philosopher‚Äôs Stone.\nThis will scrape the cast data for Harry Potter and the Philosopher‚Äôs Stone and save the results to results.csv.\n\n\n\nAfter running the recommendation system, I decided to create a simple but intuitive dataframe to find which movies and TV shows share the most actors with our target film.\nImport libraries:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt  \nfrom collections import defaultdict\n\nCreate a data frame that has two columns: Movie/TV Name and Number of Actors Shared:\n\n# Convert the scraped data in the CSV file to a data frame\nresults = pd.read_csv(\"results.csv\")\n\n# Create a dictionary to count how many times each movie/TV show appears\n# Key: Movie/TV show name, Value: Number of shared actors\nmovie_map = defaultdict(int)\n\n# Iterate over each row in the results DataFrame\nfor index, row in results.iterrows():\n    # Skip the target movie\n    if row.at[\"movie_or_TV_name\"] == \"Harry Potter and the Philosopher's Stone\":\n        continue\n    # Increment the count for the current movie/TV show\n    movie_map[row.at[\"movie_or_TV_name\"]]+=1\n\n# Convert the dictionary into a data frame\nshared_actors = pd.DataFrame(list(movie_map.items()), columns=['Movie/TV Name', 'Number of Shared Actors'])\n# Sort the data frame\nshared_actors = shared_actors.sort_values(by='Number of Shared Actors', ascending=False)\n\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie/TV Name\nNumber of Shared Actors\n\n\n\n\n102\nHarry Potter and the Chamber of Secrets\n25\n\n\n37\nHarry Potter and the Prisoner of Azkaban\n15\n\n\n72\nHarry Potter and the Deathly Hallows: Part 2\n15\n\n\n173\nDoctor Who\n12\n\n\n53\nHarry Potter and the Deathly Hallows: Part 1\n12\n\n\n...\n...\n...\n\n\n564\nMo\n1\n\n\n565\nStepping Out\n1\n\n\n566\nArtsnight\n1\n\n\n567\nBathtime\n1\n\n\n1234\n1Life\n1\n\n\n\n\n1235 rows √ó 2 columns\n\n\n\nUsing this data frame, create an intuitive visualization using Matplotlib\n\n# Create bar chart\nplt.figure(figsize=(12, 6))\nplt.barh(shared_actors[\"Movie/TV Name\"].head(10), shared_actors[\"Number of Shared Actors\"].head(10), color=\"lightblue\")  \nplt.xlabel(\"Number of Shared Actors\")  \nplt.ylabel(\"Movie / TV Show\")  \nplt.title(\"Top Movies & TV Shows Sharing Actors with Target Movie\")  \nplt.gca().invert_yaxis()  \nplt.show()  \n\n\n\n\n\n\n\n\n\n\n\nAnd that‚Äôs it for this week‚Äôs blog post! I hope you learned something about the basics of web scraping and hope to see you next week!"
  },
  {
    "objectID": "posts/HW3_WebDevelopment/index.html",
    "href": "posts/HW3_WebDevelopment/index.html",
    "title": "Web Development",
    "section": "",
    "text": "Hey there! Welcome to the fourth blog post of Python for Beginners! Today, we‚Äôre diving into an exciting project ‚Äî building a Simple Message Bank using Dash and SQLite. If you‚Äôve ever wanted to create an interactive web app where users can submit and view random messages, you‚Äôre in the right place!\nBy the end of this tutorial, you‚Äôll have a fully functional message board that lets users submit messages and randomly view them‚Äîbecause who doesn‚Äôt love a little mystery?\nLet‚Äôs get coding!\n\n\nFirst, we need a way to store and retrieve messages. That‚Äôs where SQLite comes in! Let‚Äôs import our necessary libraries, initialize the Dash app and create a function to initialize the database:\n\n# Import necessary libraries\nimport sqlite3\nimport dash\nfrom dash import dcc, html, Input, Output, State\n\n\n# Initialize the Dash app\napp = dash.Dash(__name__)\nserver = app.server\n\n\n# Function to get or create the message database\ndef get_message_db():\n    \"\"\"\n    Connects to the SQLite database or creates it if it doesn't exist.\n    Ensures that the 'messages' table is set up properly.\n    \"\"\"\n    # Check for database (message_db) defined in global scope\n    global message_db\n    # If defined, return\n    if 'message_db' in globals():\n        return message_db\n    # If not, connect to database and assign to global variable \"message_db\"\n    else:\n        db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n        cursor = db.cursor()\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                handle TEXT NOT NULL,\n                message TEXT NOT NULL\n            )\n        \"\"\")\n        db.commit()\n        return db\n\n\n\n\nWe check if the database is already open (so we don‚Äôt make multiple connections).\nIf not, we connect to SQLite and create a table named messages to store user-submitted messages.\nWe AUTOINCREMENT the id (because who wants to manually track message IDs?).\n\n\n\n\n\nNow, let‚Äôs allow users to submit messages! We need a function that will take the user‚Äôs name and message, then save it to our database.\nThis function grabs our database, inserts a new row with the user‚Äôs handle and message, and then commits & closes the connection.\n\n# Insert Message Into Database\ndef insert_message(handle, message):\n    \"\"\"\n    Inserts a new message into the database.\n    \n    Parameters:\n        handle (str): The name or handle of the user.\n        message (str): The message content.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    # Insert message into database\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    db.close()\n\n\n\n\nInstead of displaying all messages in order, let‚Äôs randomly grab a few‚Äîbecause life is more exciting when it‚Äôs unpredictable, right?\n\nFetches 5 random messages (or fewer if there aren‚Äôt that many)\nUses SQLite‚Äôs ORDER BY RANDOM() to shuffle things up!\nCloses the database connection\n\n\n# Retrieve random messages from the database\ndef random_messages(n=5):\n    \"\"\"\n    Retrieves up to n random messages from the database.\n    \n    Parameters:\n        n (int): Number of random messages to retrieve (default is 5).\n    \n    Returns:\n        list of tuples: Each tuple contains (handle, message).\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    db.close()\n    return messages\n\n\n\n\nNow this is where the fun is - getting to customize our webpage however we want! In this tutorial, I want to keep it in the theme of pink, navy and light gray.\n\nCentered layout with a light gray background.\nPink buttons because‚Ä¶ why not?\nRounded corners for that soft, modern look.\n\n\n# Layout\napp.layout = html.Div(style={\n    'font-family': 'Times New Roman, serif',\n    'text-align': 'center',\n    'background-color': '#F3F3F3',  # Light gray background\n    'height': '100vh',\n    'display': 'flex',\n    'flex-direction': 'column',\n    'justify-content': 'center',\n    'align-items': 'center'\n}, children=[\n    html.H1(\"A SIMPLE MESSAGE BANK\", style={'color': 'navy', 'font-weight': 'bold'}),\n\n    html.Div(style={\n        'background-color': 'white',\n        'padding': '20px',\n        'border-radius': '10px',\n        'box-shadow': '0px 4px 10px rgba(0, 0, 0, 0.1)',\n        'width': '50%',\n        'text-align': 'left'\n    }, children=[\n        html.H3(\"Submit\", style={'font-family': 'Times New Roman, serif', 'color': 'navy'}),\n        html.Label(\"Your Message:\", style={'color': 'black'}),\n        dcc.Input(id='message', type='text', placeholder='Enter your message',\n                  style={'font-family': 'Times New Roman, serif', 'width': '100%', 'padding': '8px', 'margin-bottom': '10px', 'border-radius': '5px', 'border': '1px solid lightgray'}),\n        \n        html.Label(\"Your Name or Handle:\", style={'color': 'black'}),\n        dcc.Input(id='handle', type='text', placeholder='Enter your name',\n                  style={'font-family': 'Times New Roman, serif', 'width': '100%', 'padding': '8px', 'margin-bottom': '10px', 'border-radius': '5px', 'border': '1px solid lightgray'}),\n        \n        html.Button('Submit', id='submit-button', n_clicks=0, style={\n            'font-family': 'Times New Roman, serif',\n            'background-color': 'pink',\n            'color': 'white',\n            'border': 'none',\n            'padding': '10px 15px',\n            'border-radius': '5px',\n            'cursor': 'pointer'\n        }),\n        html.Div(id='confirmation', style={'margin-top': '10px', 'color': 'green'})\n    ]),\n\n    html.H3(\"View\", style={'color': 'navy', 'margin-top': '20px'}),\n    html.Ul(id='messages-list', style={'list-style-type': 'none', 'padding': 0}),\n\n    html.Button('Update', id='update-button', n_clicks=0, style={\n        'font-family': 'Times New Roman, serif',\n        'background-color': 'pink',\n        'color': 'white',\n        'border': 'none',\n        'padding': '10px 15px',\n        'border-radius': '5px',\n        'cursor': 'pointer',\n        'margin-top': '10px'\n    }),\n])\n\n\n\n\nNow, let‚Äôs wire up the buttons! We need two callbacks:\n\nOne for submitting messages.\n\n\n# Callback for submitting messages\n@app.callback(\n    Output('confirmation', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('handle', 'value'), State('message', 'value')]\n)\n\ndef submit(n_clicks, handle, message):\n    \"\"\"\n    Handles message submission when the submit button is clicked.\n    \n    Parameters:\n        n_clicks (int): Number of button clicks.\n        handle (str): User's handle/name.\n        message (str): User's message.\n    \n    Returns:\n        str: Confirmation message upon successful submission.\n    \"\"\"\n    if n_clicks &gt; 0 and handle and message:\n        insert_message(handle, message)\n        return \"Thanks for submitting a message!\"\n    return \"\"\n\n\nOne for fetching random messages when users hit ‚ÄúUpdate‚Äù.\n\n\n# Callback for updating the view with random messages\n@app.callback(\n    Output('messages-list', 'children'),\n    Input('update-button', 'n_clicks')\n)\n\ndef view(n_clicks):\n    \"\"\"\n    Gets and displays random messages when the update button is clicked.\n    \n    Parameters:\n        n_clicks (int): Number of button clicks.\n    \n    Returns:\n        list: A list of HTML list items containing messages.\n    \"\"\"\n    messages = random_messages(5)\n    return [html.Li(html.I(f\"{h}: {m}\"), style={'color': 'black', 'margin-bottom': '5px'}) for h, m in messages]\n\n\n\n\nAdd this last piece of code to get your web app up and running!\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=8060)\n\n\n\n\n\nHere is an example of the user submitting a message and in the handle field, they can use either their name or their GitHub handle.\n\n\nfrom IPython.display import display, Image\n\ndisplay(Image(\"pic1.png\"))\n\n\n\n\n\n\n\n\n\nHere is an example of a user viewing submitted messages. Show at least two messages, one of which is the message you submitted in the previous screencap. This message should show your name or GitHub handle.\n\n\ndisplay(Image(\"pic1.png\"))\n\n\n\n\n\n\n\n\n\n\n\nAnd that wraps it up for our Python for Beginners: Building a Web App with Dash!"
  },
  {
    "objectID": "posts/HW3_WebDevelopment/index.html#introduction",
    "href": "posts/HW3_WebDevelopment/index.html#introduction",
    "title": "Web Development",
    "section": "",
    "text": "Hey there! Welcome to the fourth blog post of Python for Beginners! Today, we‚Äôre diving into an exciting project ‚Äî building a Simple Message Bank using Dash and SQLite. If you‚Äôve ever wanted to create an interactive web app where users can submit and view random messages, you‚Äôre in the right place!\nBy the end of this tutorial, you‚Äôll have a fully functional message board that lets users submit messages and randomly view them‚Äîbecause who doesn‚Äôt love a little mystery?\nLet‚Äôs get coding!\n\n\nFirst, we need a way to store and retrieve messages. That‚Äôs where SQLite comes in! Let‚Äôs import our necessary libraries, initialize the Dash app and create a function to initialize the database:\n\n# Import necessary libraries\nimport sqlite3\nimport dash\nfrom dash import dcc, html, Input, Output, State\n\n\n# Initialize the Dash app\napp = dash.Dash(__name__)\nserver = app.server\n\n\n# Function to get or create the message database\ndef get_message_db():\n    \"\"\"\n    Connects to the SQLite database or creates it if it doesn't exist.\n    Ensures that the 'messages' table is set up properly.\n    \"\"\"\n    # Check for database (message_db) defined in global scope\n    global message_db\n    # If defined, return\n    if 'message_db' in globals():\n        return message_db\n    # If not, connect to database and assign to global variable \"message_db\"\n    else:\n        db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n        cursor = db.cursor()\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                handle TEXT NOT NULL,\n                message TEXT NOT NULL\n            )\n        \"\"\")\n        db.commit()\n        return db\n\n\n\n\nWe check if the database is already open (so we don‚Äôt make multiple connections).\nIf not, we connect to SQLite and create a table named messages to store user-submitted messages.\nWe AUTOINCREMENT the id (because who wants to manually track message IDs?).\n\n\n\n\n\nNow, let‚Äôs allow users to submit messages! We need a function that will take the user‚Äôs name and message, then save it to our database.\nThis function grabs our database, inserts a new row with the user‚Äôs handle and message, and then commits & closes the connection.\n\n# Insert Message Into Database\ndef insert_message(handle, message):\n    \"\"\"\n    Inserts a new message into the database.\n    \n    Parameters:\n        handle (str): The name or handle of the user.\n        message (str): The message content.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    # Insert message into database\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    db.close()\n\n\n\n\nInstead of displaying all messages in order, let‚Äôs randomly grab a few‚Äîbecause life is more exciting when it‚Äôs unpredictable, right?\n\nFetches 5 random messages (or fewer if there aren‚Äôt that many)\nUses SQLite‚Äôs ORDER BY RANDOM() to shuffle things up!\nCloses the database connection\n\n\n# Retrieve random messages from the database\ndef random_messages(n=5):\n    \"\"\"\n    Retrieves up to n random messages from the database.\n    \n    Parameters:\n        n (int): Number of random messages to retrieve (default is 5).\n    \n    Returns:\n        list of tuples: Each tuple contains (handle, message).\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    db.close()\n    return messages\n\n\n\n\nNow this is where the fun is - getting to customize our webpage however we want! In this tutorial, I want to keep it in the theme of pink, navy and light gray.\n\nCentered layout with a light gray background.\nPink buttons because‚Ä¶ why not?\nRounded corners for that soft, modern look.\n\n\n# Layout\napp.layout = html.Div(style={\n    'font-family': 'Times New Roman, serif',\n    'text-align': 'center',\n    'background-color': '#F3F3F3',  # Light gray background\n    'height': '100vh',\n    'display': 'flex',\n    'flex-direction': 'column',\n    'justify-content': 'center',\n    'align-items': 'center'\n}, children=[\n    html.H1(\"A SIMPLE MESSAGE BANK\", style={'color': 'navy', 'font-weight': 'bold'}),\n\n    html.Div(style={\n        'background-color': 'white',\n        'padding': '20px',\n        'border-radius': '10px',\n        'box-shadow': '0px 4px 10px rgba(0, 0, 0, 0.1)',\n        'width': '50%',\n        'text-align': 'left'\n    }, children=[\n        html.H3(\"Submit\", style={'font-family': 'Times New Roman, serif', 'color': 'navy'}),\n        html.Label(\"Your Message:\", style={'color': 'black'}),\n        dcc.Input(id='message', type='text', placeholder='Enter your message',\n                  style={'font-family': 'Times New Roman, serif', 'width': '100%', 'padding': '8px', 'margin-bottom': '10px', 'border-radius': '5px', 'border': '1px solid lightgray'}),\n        \n        html.Label(\"Your Name or Handle:\", style={'color': 'black'}),\n        dcc.Input(id='handle', type='text', placeholder='Enter your name',\n                  style={'font-family': 'Times New Roman, serif', 'width': '100%', 'padding': '8px', 'margin-bottom': '10px', 'border-radius': '5px', 'border': '1px solid lightgray'}),\n        \n        html.Button('Submit', id='submit-button', n_clicks=0, style={\n            'font-family': 'Times New Roman, serif',\n            'background-color': 'pink',\n            'color': 'white',\n            'border': 'none',\n            'padding': '10px 15px',\n            'border-radius': '5px',\n            'cursor': 'pointer'\n        }),\n        html.Div(id='confirmation', style={'margin-top': '10px', 'color': 'green'})\n    ]),\n\n    html.H3(\"View\", style={'color': 'navy', 'margin-top': '20px'}),\n    html.Ul(id='messages-list', style={'list-style-type': 'none', 'padding': 0}),\n\n    html.Button('Update', id='update-button', n_clicks=0, style={\n        'font-family': 'Times New Roman, serif',\n        'background-color': 'pink',\n        'color': 'white',\n        'border': 'none',\n        'padding': '10px 15px',\n        'border-radius': '5px',\n        'cursor': 'pointer',\n        'margin-top': '10px'\n    }),\n])\n\n\n\n\nNow, let‚Äôs wire up the buttons! We need two callbacks:\n\nOne for submitting messages.\n\n\n# Callback for submitting messages\n@app.callback(\n    Output('confirmation', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('handle', 'value'), State('message', 'value')]\n)\n\ndef submit(n_clicks, handle, message):\n    \"\"\"\n    Handles message submission when the submit button is clicked.\n    \n    Parameters:\n        n_clicks (int): Number of button clicks.\n        handle (str): User's handle/name.\n        message (str): User's message.\n    \n    Returns:\n        str: Confirmation message upon successful submission.\n    \"\"\"\n    if n_clicks &gt; 0 and handle and message:\n        insert_message(handle, message)\n        return \"Thanks for submitting a message!\"\n    return \"\"\n\n\nOne for fetching random messages when users hit ‚ÄúUpdate‚Äù.\n\n\n# Callback for updating the view with random messages\n@app.callback(\n    Output('messages-list', 'children'),\n    Input('update-button', 'n_clicks')\n)\n\ndef view(n_clicks):\n    \"\"\"\n    Gets and displays random messages when the update button is clicked.\n    \n    Parameters:\n        n_clicks (int): Number of button clicks.\n    \n    Returns:\n        list: A list of HTML list items containing messages.\n    \"\"\"\n    messages = random_messages(5)\n    return [html.Li(html.I(f\"{h}: {m}\"), style={'color': 'black', 'margin-bottom': '5px'}) for h, m in messages]\n\n\n\n\nAdd this last piece of code to get your web app up and running!\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=8060)\n\n\n\n\n\nHere is an example of the user submitting a message and in the handle field, they can use either their name or their GitHub handle.\n\n\nfrom IPython.display import display, Image\n\ndisplay(Image(\"pic1.png\"))\n\n\n\n\n\n\n\n\n\nHere is an example of a user viewing submitted messages. Show at least two messages, one of which is the message you submitted in the previous screencap. This message should show your name or GitHub handle.\n\n\ndisplay(Image(\"pic1.png\"))\n\n\n\n\n\n\n\n\n\n\n\nAnd that wraps it up for our Python for Beginners: Building a Web App with Dash!"
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html",
    "href": "posts/HW6_TextClassification/index.html",
    "title": "Text Classification",
    "section": "",
    "text": "Hello again! Welcome to the last blog post of our Python for Beginners series! Today, we will be using Text Classification to help solve one of the most pressing challenges of the digital age ‚Äî fake news.\nMisinformation from fake news spreads like wildfire, shaping public opinion and even influencing democratic processes along the way.\nBut can we use AI to help us fight back?\nIn this post, we‚Äôll build and evaluate a fake news classifier using Keras, using text classification to determine whether a news article is real or not.\nOur dataset comes from research by Ahmed, Traore, and Saad (2017), who explored fake news detection using N-Gram Analysis and Machine Learning. We‚Äôll take their findings a step further by using deep learning to train models that analyze article titles, full text, or both.\nLet‚Äôs get started and see if AI can separate fact from fiction!\n\n\nBefore we dive into building our fake news classifier, let‚Äôs start by importing all the necessary libraries. These will help us with everything from data processing to model training and evaluation.\n\n# Import libraries\nimport re\nimport nltk\nimport keras\nimport string\nimport numpy as np\nimport pandas as pd\nfrom keras import utils\nimport tensorflow as tf\nfrom keras import layers\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport tensorflow_datasets as tfds\nfrom sklearn.decomposition import PCA\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, TextVectorization, Embedding, Dense, Dropout, GlobalAveragePooling1D, Concatenate\n\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n\n\nNow that we have set up our environment, it‚Äôs time to load in our data! Our dataset can be accessed in two easy ways:\n\nRead it directly into Python using pd.read_csv()\nDownload it to your computer and load it from disk\n\nTo keep it simple, let‚Äôs go with the first approach:\n\n# URL to the dataset\ntrain_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_train.csv\"\n\n\n# Pandas version of dataset\ndata = pd.read_csv(train_url)\n\nLet‚Äôs take a quick peek to see what our data looks like:\n\n# Visualize dataset\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and ‚ÄúClose Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThis dataset is structured as follows:\n\ntitle ‚Äì The headline of the article.\ntext ‚Äì The full body of the article.\nfake ‚Äì A binary label:\n\n0 ‚Üí The article is real.\n1 ‚Üí The article is fake.\n\n\n\n\n\nBefore we can train our model, we have to first prepare our dataset. We will clean and transform the raw text data into a format that TensorFlow can process efficiently. To do this, we create a function called make_dataset that preprocesses our data:\n\nText Cleaning ‚Äì The function first standardizes the text by converting it to lowercase, and filtering out stopwords (common words like ‚Äòthe‚Äô, ‚Äòand‚Äô, ‚Äòbut‚Äô, that don‚Äôt add meaning).\nDataset Construction ‚Äì It then converts the processed data into a tf.data.Dataset, with each sample consisting of two inputs: the title and the article text, and one output: the fake news label (0 for real news, 1 for fake news).\nBatching for Efficiency ‚Äì Finally, the dataset is shuffled and batched to improve training speed. A batch size of 100 is used to balance performance and accuracy.\n\nHere is how to implement the function:\n\n# Make Dataset Function\ndef make_dataset(url):\n  # Read dataset to Pandas\n  df = pd.read_csv(url)\n\n  # Make title and text lowercase\n  df[\"title\"] = df[\"title\"].str.lower()\n  df[\"text\"] = df[\"text\"].str.lower()\n\n  # df column names\n  df.columns = [\"Unnamed: 0\", \"title\", \"text\", \"fake\"]\n\n  # Remove stopwords in title and text\n  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n\n  # Get inputs (title, text) and output (fake label)\n  titles = df['title'].values\n  texts = df['text'].values\n  labels = df['fake'].values.astype(np.int32)\n\n  # Create a Dataset\n  dataset = tf.data.Dataset.from_tensor_slices(((titles, texts), labels))\n  dataset = dataset.batch(100).prefetch(tf.data.experimental.AUTOTUNE)\n\n  # Return\n  return dataset\n\n\n# Dataset\ndata = make_dataset(train_url)\n\n\n\n\nTo make sure our model generalizes well to unseen data, we need to split our dataset into training (80%) and validation (20%) sets. The training set is used to optimize the model, while the validation set helps us evaluate performance and detect overfitting.\nSteps: 1. Count the Total Samples ‚Äì Since tf.data.Dataset does not have a built-in len() function, we use the reduce() method to iterate through the dataset and count its elements. 2. Determine Split Sizes ‚Äì We calculate 80% of the total dataset size for training, which leaves 20% for validation. 3. Use take() and skip() ‚Äì We use .take(train_size) to get the first 80% of the dataset for training and .skip(train_size) to get the remaining 20% for validation.\nHere‚Äôs the implementation:\n\n# Split the dataset into training (80%) and validation (20%)\n\n# Count total elements in dataset\ndataset_size = data.reduce(0, lambda x, _: x + 1).numpy()\n\n# Define split sizes\ntrain_size = int(0.8 * dataset_size)\n\n# Split dataset\n# Take first 80% of the dataset\ntrain_ds = data.take(train_size)\n# Skip first 80% and take remaining 20%\nval_ds = data.skip(train_size)\n\n\n\n\nAnother step before training our model is to determine the base rate accuracy ‚Äî the accuracy a model would achieve by always predicting the most frequent class. This gives us a benchmark to compare our trained model against.\nSteps:\n\nExtract Labels ‚Äì Since our dataset is in tf.data.Dataset format, we need to extract the labels using .unbatch() and .map().\nCount the Labels ‚Äì We count how many articles are labeled as true (0) and how many are labeled as fake (1).\nCalculate Baseline Accuracy ‚Äì The base rate is the proportion of the majority class in the dataset.\n\n\n# Labels iterator\nlabels_iterator = train_ds.unbatch().map(lambda inputs, label: label).as_numpy_iterator()\n\n\n# Examine labels\nlabels = np.array(list(labels_iterator))\n\n# Count true and false\nnum_true = np.sum(labels == 0)\nnum_false = np.sum(labels == 1)\n\n# Print\nprint(f\"Number of True Articles: {num_true}\")\nprint(f\"Number of False Articles: {num_false}\")\n\nNumber of True Articles: 8603\nNumber of False Articles: 9397\n\n\n\n# Baseline Accuracy\ntotal = num_true + num_false\nbaseline = (max(num_true, num_false) / total) * 100\nprint(f\"Baseline accuracy: {baseline:.2f}%\")\n\nBaseline accuracy: 52.21%\n\n\n\n\n\nNow, it‚Äôs time to get to the good stuff! In natural language processing (NLP), the way we preprocess and encode textual data impacts model performance. To build our fake news detection model, we have multiple inputs, such as titles and text. Usually, these inputs are processed separately, but a shared vectorization and embedding layer makes sure our models are consistent (same words have the same vector representation in title and text) and efficient (eliminates redundance embedding layers to reduce memory usage and model complexity).\nWe will implement a shared vectorization and embedding layer to power three different models:\n\nTitle-Only Model\nText-Only Model\nCombined Title + Text Model\n\nHere is the implementation:\n\n# Preparing a shared text vectorization layer for tf model\nsize_vocabulary = 2000\n\n# Standardization function\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation\n\n# Shared Text Vectorization Layer\nshared_vectorize_layer = layers.TextVectorization(\n                                                  standardize=standardization,\n                                                  max_tokens=size_vocabulary,\n                                                  output_mode='int',\n                                                  output_sequence_length=500\n                                                 )\n# Access both title and text\nshared_vectorize_layer.adapt(train_ds.map(lambda x, y: tf.concat([x[0], x[1]], axis=0)))\n\n\n# Shared Embedding Layer\nshared_embedding = layers.Embedding(input_dim=size_vocabulary, output_dim=16)\n\nHere, shared_vectorize_layer tokenizes and converts input text into numerical sequences, while shared_embedding_layer maps these sequences into dense vector representations.\n\n\n\n# Functional API for Title Model\n# Title Input Layer\ntitle_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title_input\")\n\n# Shared vectorization and embedding layers\ntitle_vectors = shared_vectorize_layer(title_input)\ntitle_embeddings = shared_embedding(title_vectors)\n\n# Rest of the layers\n# Converts sequence into a single vector\nx = layers.GlobalAveragePooling1D()(title_embeddings)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\n# Output layer\n# Binary classification\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Define the model\ntitle_model = keras.Model(inputs=title_input, outputs=output, name=\"Title_Only_Model\")\n\n\n# Compile the model\ntitle_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\n\n# Train the Model\nhistory = title_model.fit(\n    # Extract title and label\n    train_ds.map(lambda x, y: (x[0], y)),\n    # Extract title and label\n    validation_data=val_ds.map(lambda x, y: (x[0], y)),\n    epochs=20,\n    batch_size=100\n)\n\nEpoch 1/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 12ms/step - accuracy: 0.5145 - loss: 0.6925 - val_accuracy: 0.5266 - val_loss: 0.6908\nEpoch 2/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 11ms/step - accuracy: 0.5357 - loss: 0.6899 - val_accuracy: 0.7341 - val_loss: 0.6796\nEpoch 3/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.6034 - loss: 0.6693 - val_accuracy: 0.7620 - val_loss: 0.6087\nEpoch 4/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 16ms/step - accuracy: 0.6937 - loss: 0.5981 - val_accuracy: 0.7732 - val_loss: 0.5148\nEpoch 5/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 10ms/step - accuracy: 0.7486 - loss: 0.5207 - val_accuracy: 0.7946 - val_loss: 0.4619\nEpoch 6/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.7621 - loss: 0.4876 - val_accuracy: 0.8038 - val_loss: 0.4303\nEpoch 7/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.7840 - loss: 0.4535 - val_accuracy: 0.8197 - val_loss: 0.4069\nEpoch 8/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 14ms/step - accuracy: 0.8038 - loss: 0.4213 - val_accuracy: 0.8296 - val_loss: 0.3875\nEpoch 9/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 12ms/step - accuracy: 0.8075 - loss: 0.4140 - val_accuracy: 0.8370 - val_loss: 0.3677\nEpoch 10/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8175 - loss: 0.3960 - val_accuracy: 0.8496 - val_loss: 0.3479\nEpoch 11/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 10ms/step - accuracy: 0.8335 - loss: 0.3696 - val_accuracy: 0.8521 - val_loss: 0.3354\nEpoch 12/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 10ms/step - accuracy: 0.8394 - loss: 0.3590 - val_accuracy: 0.8535 - val_loss: 0.3226\nEpoch 13/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 13ms/step - accuracy: 0.8496 - loss: 0.3385 - val_accuracy: 0.8735 - val_loss: 0.3024\nEpoch 14/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 12ms/step - accuracy: 0.8556 - loss: 0.3256 - val_accuracy: 0.8753 - val_loss: 0.2905\nEpoch 15/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8663 - loss: 0.3069 - val_accuracy: 0.8764 - val_loss: 0.2824\nEpoch 16/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8740 - loss: 0.2924 - val_accuracy: 0.8714 - val_loss: 0.2846\nEpoch 17/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8749 - loss: 0.2875 - val_accuracy: 0.8708 - val_loss: 0.2804\nEpoch 18/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 10ms/step - accuracy: 0.8775 - loss: 0.2816 - val_accuracy: 0.8998 - val_loss: 0.2482\nEpoch 19/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 15ms/step - accuracy: 0.8829 - loss: 0.2722 - val_accuracy: 0.8973 - val_loss: 0.2459\nEpoch 20/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 10ms/step - accuracy: 0.8891 - loss: 0.2575 - val_accuracy: 0.9007 - val_loss: 0.2384\n\n\n\n\n\n# Evaluate model\nval_loss, val_acc = title_model.evaluate(val_ds.map(lambda x, y: (x[0], y)))\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\n\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - accuracy: 0.9026 - loss: 0.2368\nValidation Accuracy: 0.90\n\n\n\n\n\nWe will create a function that we can use for the rest of the models to plot Training and Validation Accuracy/Loss to keep track of how our models do throughout the epochs.\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Title-Only Model: Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Title-Only Model: Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualization for title model\nutils.plot_model(title_model, \"title_visualization.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\nThe title-only model achieving 90% validation accuracy is quite impressive, considering the fact that it is only using the title of the article. This suggests that the title alone must contain a lot of important information for distinguishing between real vs.¬†fake news. The model is effectively capturing key features from just a short, high-level summary of the content, which is a positive indicator of the model‚Äôs ability to generalize from minimal information.\n\n\n\n\n\n# Functional API for Text Model\n# Text Input Layer\ntext_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text_input\")\n\n# Shared vectorization and embedding layers\ntext_vectors = shared_vectorize_layer(text_input)\ntext_embeddings = shared_embedding(text_vectors)\n\n# Rest of the layers\n# Converts sequence into a single vector\nx = layers.GlobalAveragePooling1D()(text_embeddings)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\n# Output layer\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Define the model\ntext_model = keras.Model(inputs=text_input, outputs=output, name=\"Text_Only_Model\")\n\n\n# Compile the model\ntext_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\n\n# Train the Model\nhistory = text_model.fit(\n    train_ds.map(lambda x, y: (x[1], y)), # Extract text and label\n    validation_data=val_ds.map(lambda x, y: (x[1], y)), # Extract text and label\n    epochs=20,  # Adjust epochs based on overfitting\n    batch_size=100\n)\n\nEpoch 1/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 24ms/step - accuracy: 0.6196 - loss: 0.6552 - val_accuracy: 0.8546 - val_loss: 0.4592\nEpoch 2/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.8534 - loss: 0.4076 - val_accuracy: 0.8887 - val_loss: 0.2883\nEpoch 3/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 23ms/step - accuracy: 0.9047 - loss: 0.2692 - val_accuracy: 0.9310 - val_loss: 0.2111\nEpoch 4/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 28ms/step - accuracy: 0.9288 - loss: 0.2141 - val_accuracy: 0.9562 - val_loss: 0.1735\nEpoch 5/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 19ms/step - accuracy: 0.9405 - loss: 0.1852 - val_accuracy: 0.9616 - val_loss: 0.1502\nEpoch 6/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9486 - loss: 0.1616 - val_accuracy: 0.9622 - val_loss: 0.1378\nEpoch 7/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.9497 - loss: 0.1484 - val_accuracy: 0.9658 - val_loss: 0.1237\nEpoch 8/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 18ms/step - accuracy: 0.9554 - loss: 0.1360 - val_accuracy: 0.9622 - val_loss: 0.1193\nEpoch 9/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 20ms/step - accuracy: 0.9563 - loss: 0.1270 - val_accuracy: 0.9694 - val_loss: 0.1072\nEpoch 10/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 18ms/step - accuracy: 0.9607 - loss: 0.1166 - val_accuracy: 0.9703 - val_loss: 0.1013\nEpoch 11/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 24ms/step - accuracy: 0.9664 - loss: 0.1084 - val_accuracy: 0.9717 - val_loss: 0.0974\nEpoch 12/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.9664 - loss: 0.1034 - val_accuracy: 0.9726 - val_loss: 0.0929\nEpoch 13/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 18ms/step - accuracy: 0.9686 - loss: 0.0978 - val_accuracy: 0.9712 - val_loss: 0.0899\nEpoch 14/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 21ms/step - accuracy: 0.9691 - loss: 0.0926 - val_accuracy: 0.9726 - val_loss: 0.0859\nEpoch 15/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 18ms/step - accuracy: 0.9717 - loss: 0.0884 - val_accuracy: 0.9733 - val_loss: 0.0833\nEpoch 16/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 24ms/step - accuracy: 0.9730 - loss: 0.0840 - val_accuracy: 0.9733 - val_loss: 0.0815\nEpoch 17/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.9749 - loss: 0.0794 - val_accuracy: 0.9744 - val_loss: 0.0802\nEpoch 18/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 19ms/step - accuracy: 0.9764 - loss: 0.0756 - val_accuracy: 0.9751 - val_loss: 0.0774\nEpoch 19/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 21ms/step - accuracy: 0.9771 - loss: 0.0710 - val_accuracy: 0.9753 - val_loss: 0.0753\nEpoch 20/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 18ms/step - accuracy: 0.9775 - loss: 0.0681 - val_accuracy: 0.9742 - val_loss: 0.0751\n\n\n\n\n\n# Evaluate model\nval_loss, val_acc = text_model.evaluate(val_ds.map(lambda x, y: (x[1], y)))\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\n\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 16ms/step - accuracy: 0.9777 - loss: 0.0740\nValidation Accuracy: 0.97\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Text-Only Model: Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Text-Only Model: Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualization for title model\nutils.plot_model(text_model, \"text_visualization.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\nThe text-only model achieving a 97% validation accuracy is a sizable improvement over the title-only model‚Äôs 90%. This suggests that the full text provides more valuable information for distinguishing between classes like real vs.¬†fake news compared to just the title. The increase in accuracy means that the body of the text contains additional context, details, and subtle cues that the model can use to make more informed predictions.\n\n\n\n\n\n# Title Input\ntitle_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title_input\")\ntitle_vectors = shared_vectorize_layer(title_input)\ntitle_embeddings = shared_embedding(title_vectors)\ntitle_output = layers.GlobalAveragePooling1D()(title_embeddings)\n\n# Text Input\ntext_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text_input\")\ntext_vectors = shared_vectorize_layer(text_input)\ntext_embeddings = shared_embedding(text_vectors)\ntext_output = layers.GlobalAveragePooling1D()(text_embeddings)\n\n# Combinining inputs\nmerged = layers.concatenate([title_output, text_output])\n\n# Fully connected layers\nx = layers.Dense(64, activation=\"relu\")(merged)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\n# Output Layer\nfinal_output = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Define the model\ncombined_model = keras.Model(inputs=[title_input, text_input], outputs=final_output, name=\"Title_Text_Model\")\n\n\n# Compile Model\ncombined_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\n\n# Train Model\nhistory = combined_model.fit(\n    # Get title, text, and labels\n    train_ds.map(lambda x, y: ((x[0], x[1]), y)),\n    # Get title, text, and labels\n    validation_data=val_ds.map(lambda x, y: ((x[0], x[1]), y)),\n    epochs=20,\n    batch_size=100\n)\n\nEpoch 1/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 31ms/step - accuracy: 0.7980 - loss: 0.4460 - val_accuracy: 0.9634 - val_loss: 0.1102\nEpoch 2/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 25ms/step - accuracy: 0.9672 - loss: 0.1036 - val_accuracy: 0.9616 - val_loss: 0.1036\nEpoch 3/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 32ms/step - accuracy: 0.9703 - loss: 0.0873 - val_accuracy: 0.9676 - val_loss: 0.0893\nEpoch 4/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9803 - loss: 0.0649 - val_accuracy: 0.9742 - val_loss: 0.0736\nEpoch 5/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9823 - loss: 0.0571 - val_accuracy: 0.9652 - val_loss: 0.0936\nEpoch 6/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 25ms/step - accuracy: 0.9817 - loss: 0.0545 - val_accuracy: 0.9753 - val_loss: 0.0707\nEpoch 7/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7s 40ms/step - accuracy: 0.9863 - loss: 0.0467 - val_accuracy: 0.9762 - val_loss: 0.0659\nEpoch 8/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 33ms/step - accuracy: 0.9860 - loss: 0.0423 - val_accuracy: 0.9836 - val_loss: 0.0548\nEpoch 9/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 30ms/step - accuracy: 0.9872 - loss: 0.0392 - val_accuracy: 0.9827 - val_loss: 0.0567\nEpoch 10/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 25ms/step - accuracy: 0.9854 - loss: 0.0412 - val_accuracy: 0.9665 - val_loss: 0.0922\nEpoch 11/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 33ms/step - accuracy: 0.9874 - loss: 0.0385 - val_accuracy: 0.9748 - val_loss: 0.0717\nEpoch 12/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 27ms/step - accuracy: 0.9849 - loss: 0.0417 - val_accuracy: 0.9861 - val_loss: 0.0494\nEpoch 13/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9910 - loss: 0.0277 - val_accuracy: 0.9845 - val_loss: 0.0515\nEpoch 14/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 31ms/step - accuracy: 0.9911 - loss: 0.0270 - val_accuracy: 0.9836 - val_loss: 0.0576\nEpoch 15/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9919 - loss: 0.0262 - val_accuracy: 0.9854 - val_loss: 0.0517\nEpoch 16/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 32ms/step - accuracy: 0.9935 - loss: 0.0215 - val_accuracy: 0.9847 - val_loss: 0.0541\nEpoch 17/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9932 - loss: 0.0223 - val_accuracy: 0.9834 - val_loss: 0.0547\nEpoch 18/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 25ms/step - accuracy: 0.9932 - loss: 0.0233 - val_accuracy: 0.9766 - val_loss: 0.0727\nEpoch 19/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 32ms/step - accuracy: 0.9909 - loss: 0.0256 - val_accuracy: 0.9843 - val_loss: 0.0562\nEpoch 20/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9934 - loss: 0.0219 - val_accuracy: 0.9840 - val_loss: 0.0565\n\n\n\n\n\nval_loss, val_acc = combined_model.evaluate(\n    val_ds.map(lambda x, y: ((x[0], x[1]), y))\n)\n\n# Print Validation Accuracy\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\n\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 14ms/step - accuracy: 0.9872 - loss: 0.0561\nValidation Accuracy: 0.98\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Combined Model: Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Combined Model: Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualization for title model\nutils.plot_model(combined_model, \"combined_visualization.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\nThe combined model achieving a 98% validation accuracy shows an improvement of 8% over the title-only model, but only a 1% improvement over the text-only model. This suggests that while combining the title and the full text provides some added value, the majority of the performance boost likely comes from the full text itself.\nThe large jump from the title-only model to the combined model reflects how the title can still contribute useful information, even though it may not be as rich as the full article. The fact that the improvement over the text-only model is only 1% implies that the full text already captures most of the necessary features for accurate classification, and adding the title does not introduce much additional useful information.\n\n\n\n\n\nNow that we‚Äôve fine-tuned our model and optimized its performance on validation data, it‚Äôs time for the ultimate test: unseen data. Evaluating our best model on fresh, unencountered samples will help us see how well our model generalizes beyond the training set.\n\n\n\ntest_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_test.csv\"\n\nTo have consistent results, we‚Äôll preprocess this data using the same make_dataset function we defined earlier. This will standardize formatting, tokenize the text appropriately, and prepare it for evaluation.\n\n# Dataset\ntest_data = make_dataset(test_url)\n\n\n\n\n\n# Evaluate the model\ntest_loss, test_accuracy = combined_model.evaluate(test_data)\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n\n# Evaluate the model on validation data\nval_loss, val_accuracy = combined_model.evaluate(val_ds)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\n225/225 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 11ms/step - accuracy: 0.9842 - loss: 0.0576\nTest Accuracy: 0.9833\nTest Loss: 0.0595\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 11ms/step - accuracy: 0.9872 - loss: 0.0561\nValidation Accuracy: 0.9840\nValidation Loss: 0.0565\n\n\n\nweights = combined_model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = shared_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\n\nweights = combined_model.get_layer('embedding').get_weights()[0]\n\n\nprint(weights.shape)\n\n(2000, 16)\n\n\n\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\n\nweights.shape\n\n(2000, 2)\n\n\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nembedding_df\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n\n0.458991\n-0.369868\n\n\n1\n[UNK]\n-3.254118\n3.480346\n\n\n2\nsaid\n11.903173\n7.233503\n\n\n3\ntrump\n-7.662038\n3.243038\n\n\n4\nus\n13.215361\n-7.409301\n\n\n...\n...\n...\n...\n\n\n1995\nreaction\n-4.046322\n-2.003912\n\n\n1996\nministers\n3.829593\n-2.292025\n\n\n1997\nwonder\n-6.529799\n-4.318990\n\n\n1998\nsetting\n-0.664706\n0.334590\n\n\n1999\nsector\n1.703211\n0.525156\n\n\n\n\n2000 rows √ó 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\nThese words are all about structure, planning, and organization. ‚ÄúLaw‚Äù and ‚ÄúPolitics‚Äù are both about how society is run, dealing with rules, policies, and decisions. ‚ÄúOrganization‚Äù and ‚ÄúProjects‚Äù are about groups or actions focused on reaching specific goals through planning and teamwork. The word ‚Äúgoal‚Äù connects everything, since all the other words are about setting and achieving goals, whether it‚Äôs in politics, law, organizations, or projects. Their closeness shows they all involve systems or efforts to make things happen, which is why they are similar."
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#set-up-project",
    "href": "posts/HW6_TextClassification/index.html#set-up-project",
    "title": "Text Classification",
    "section": "",
    "text": "Before we dive into building our fake news classifier, let‚Äôs start by importing all the necessary libraries. These will help us with everything from data processing to model training and evaluation.\n\n# Import libraries\nimport re\nimport nltk\nimport keras\nimport string\nimport numpy as np\nimport pandas as pd\nfrom keras import utils\nimport tensorflow as tf\nfrom keras import layers\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport tensorflow_datasets as tfds\nfrom sklearn.decomposition import PCA\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, TextVectorization, Embedding, Dense, Dropout, GlobalAveragePooling1D, Concatenate\n\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!"
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#step-1-acquiring-training-data",
    "href": "posts/HW6_TextClassification/index.html#step-1-acquiring-training-data",
    "title": "Text Classification",
    "section": "",
    "text": "Now that we have set up our environment, it‚Äôs time to load in our data! Our dataset can be accessed in two easy ways:\n\nRead it directly into Python using pd.read_csv()\nDownload it to your computer and load it from disk\n\nTo keep it simple, let‚Äôs go with the first approach:\n\n# URL to the dataset\ntrain_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_train.csv\"\n\n\n# Pandas version of dataset\ndata = pd.read_csv(train_url)\n\nLet‚Äôs take a quick peek to see what our data looks like:\n\n# Visualize dataset\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and ‚ÄúClose Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThis dataset is structured as follows:\n\ntitle ‚Äì The headline of the article.\ntext ‚Äì The full body of the article.\nfake ‚Äì A binary label:\n\n0 ‚Üí The article is real.\n1 ‚Üí The article is fake."
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#step-2-make-dataset-function",
    "href": "posts/HW6_TextClassification/index.html#step-2-make-dataset-function",
    "title": "Text Classification",
    "section": "",
    "text": "Before we can train our model, we have to first prepare our dataset. We will clean and transform the raw text data into a format that TensorFlow can process efficiently. To do this, we create a function called make_dataset that preprocesses our data:\n\nText Cleaning ‚Äì The function first standardizes the text by converting it to lowercase, and filtering out stopwords (common words like ‚Äòthe‚Äô, ‚Äòand‚Äô, ‚Äòbut‚Äô, that don‚Äôt add meaning).\nDataset Construction ‚Äì It then converts the processed data into a tf.data.Dataset, with each sample consisting of two inputs: the title and the article text, and one output: the fake news label (0 for real news, 1 for fake news).\nBatching for Efficiency ‚Äì Finally, the dataset is shuffled and batched to improve training speed. A batch size of 100 is used to balance performance and accuracy.\n\nHere is how to implement the function:\n\n# Make Dataset Function\ndef make_dataset(url):\n  # Read dataset to Pandas\n  df = pd.read_csv(url)\n\n  # Make title and text lowercase\n  df[\"title\"] = df[\"title\"].str.lower()\n  df[\"text\"] = df[\"text\"].str.lower()\n\n  # df column names\n  df.columns = [\"Unnamed: 0\", \"title\", \"text\", \"fake\"]\n\n  # Remove stopwords in title and text\n  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n\n  # Get inputs (title, text) and output (fake label)\n  titles = df['title'].values\n  texts = df['text'].values\n  labels = df['fake'].values.astype(np.int32)\n\n  # Create a Dataset\n  dataset = tf.data.Dataset.from_tensor_slices(((titles, texts), labels))\n  dataset = dataset.batch(100).prefetch(tf.data.experimental.AUTOTUNE)\n\n  # Return\n  return dataset\n\n\n# Dataset\ndata = make_dataset(train_url)"
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#step-3-split-data-for-validation",
    "href": "posts/HW6_TextClassification/index.html#step-3-split-data-for-validation",
    "title": "Text Classification",
    "section": "",
    "text": "To make sure our model generalizes well to unseen data, we need to split our dataset into training (80%) and validation (20%) sets. The training set is used to optimize the model, while the validation set helps us evaluate performance and detect overfitting.\nSteps: 1. Count the Total Samples ‚Äì Since tf.data.Dataset does not have a built-in len() function, we use the reduce() method to iterate through the dataset and count its elements. 2. Determine Split Sizes ‚Äì We calculate 80% of the total dataset size for training, which leaves 20% for validation. 3. Use take() and skip() ‚Äì We use .take(train_size) to get the first 80% of the dataset for training and .skip(train_size) to get the remaining 20% for validation.\nHere‚Äôs the implementation:\n\n# Split the dataset into training (80%) and validation (20%)\n\n# Count total elements in dataset\ndataset_size = data.reduce(0, lambda x, _: x + 1).numpy()\n\n# Define split sizes\ntrain_size = int(0.8 * dataset_size)\n\n# Split dataset\n# Take first 80% of the dataset\ntrain_ds = data.take(train_size)\n# Skip first 80% and take remaining 20%\nval_ds = data.skip(train_size)"
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#step-4-base-rate",
    "href": "posts/HW6_TextClassification/index.html#step-4-base-rate",
    "title": "Text Classification",
    "section": "",
    "text": "Another step before training our model is to determine the base rate accuracy ‚Äî the accuracy a model would achieve by always predicting the most frequent class. This gives us a benchmark to compare our trained model against.\nSteps:\n\nExtract Labels ‚Äì Since our dataset is in tf.data.Dataset format, we need to extract the labels using .unbatch() and .map().\nCount the Labels ‚Äì We count how many articles are labeled as true (0) and how many are labeled as fake (1).\nCalculate Baseline Accuracy ‚Äì The base rate is the proportion of the majority class in the dataset.\n\n\n# Labels iterator\nlabels_iterator = train_ds.unbatch().map(lambda inputs, label: label).as_numpy_iterator()\n\n\n# Examine labels\nlabels = np.array(list(labels_iterator))\n\n# Count true and false\nnum_true = np.sum(labels == 0)\nnum_false = np.sum(labels == 1)\n\n# Print\nprint(f\"Number of True Articles: {num_true}\")\nprint(f\"Number of False Articles: {num_false}\")\n\nNumber of True Articles: 8603\nNumber of False Articles: 9397\n\n\n\n# Baseline Accuracy\ntotal = num_true + num_false\nbaseline = (max(num_true, num_false) / total) * 100\nprint(f\"Baseline accuracy: {baseline:.2f}%\")\n\nBaseline accuracy: 52.21%"
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#step-5-creating-models",
    "href": "posts/HW6_TextClassification/index.html#step-5-creating-models",
    "title": "Text Classification",
    "section": "",
    "text": "Now, it‚Äôs time to get to the good stuff! In natural language processing (NLP), the way we preprocess and encode textual data impacts model performance. To build our fake news detection model, we have multiple inputs, such as titles and text. Usually, these inputs are processed separately, but a shared vectorization and embedding layer makes sure our models are consistent (same words have the same vector representation in title and text) and efficient (eliminates redundance embedding layers to reduce memory usage and model complexity).\nWe will implement a shared vectorization and embedding layer to power three different models:\n\nTitle-Only Model\nText-Only Model\nCombined Title + Text Model\n\nHere is the implementation:\n\n# Preparing a shared text vectorization layer for tf model\nsize_vocabulary = 2000\n\n# Standardization function\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation\n\n# Shared Text Vectorization Layer\nshared_vectorize_layer = layers.TextVectorization(\n                                                  standardize=standardization,\n                                                  max_tokens=size_vocabulary,\n                                                  output_mode='int',\n                                                  output_sequence_length=500\n                                                 )\n# Access both title and text\nshared_vectorize_layer.adapt(train_ds.map(lambda x, y: tf.concat([x[0], x[1]], axis=0)))\n\n\n# Shared Embedding Layer\nshared_embedding = layers.Embedding(input_dim=size_vocabulary, output_dim=16)\n\nHere, shared_vectorize_layer tokenizes and converts input text into numerical sequences, while shared_embedding_layer maps these sequences into dense vector representations.\n\n\n\n# Functional API for Title Model\n# Title Input Layer\ntitle_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title_input\")\n\n# Shared vectorization and embedding layers\ntitle_vectors = shared_vectorize_layer(title_input)\ntitle_embeddings = shared_embedding(title_vectors)\n\n# Rest of the layers\n# Converts sequence into a single vector\nx = layers.GlobalAveragePooling1D()(title_embeddings)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\n# Output layer\n# Binary classification\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Define the model\ntitle_model = keras.Model(inputs=title_input, outputs=output, name=\"Title_Only_Model\")\n\n\n# Compile the model\ntitle_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\n\n# Train the Model\nhistory = title_model.fit(\n    # Extract title and label\n    train_ds.map(lambda x, y: (x[0], y)),\n    # Extract title and label\n    validation_data=val_ds.map(lambda x, y: (x[0], y)),\n    epochs=20,\n    batch_size=100\n)\n\nEpoch 1/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 12ms/step - accuracy: 0.5145 - loss: 0.6925 - val_accuracy: 0.5266 - val_loss: 0.6908\nEpoch 2/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 11ms/step - accuracy: 0.5357 - loss: 0.6899 - val_accuracy: 0.7341 - val_loss: 0.6796\nEpoch 3/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.6034 - loss: 0.6693 - val_accuracy: 0.7620 - val_loss: 0.6087\nEpoch 4/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 16ms/step - accuracy: 0.6937 - loss: 0.5981 - val_accuracy: 0.7732 - val_loss: 0.5148\nEpoch 5/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 10ms/step - accuracy: 0.7486 - loss: 0.5207 - val_accuracy: 0.7946 - val_loss: 0.4619\nEpoch 6/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.7621 - loss: 0.4876 - val_accuracy: 0.8038 - val_loss: 0.4303\nEpoch 7/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.7840 - loss: 0.4535 - val_accuracy: 0.8197 - val_loss: 0.4069\nEpoch 8/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 14ms/step - accuracy: 0.8038 - loss: 0.4213 - val_accuracy: 0.8296 - val_loss: 0.3875\nEpoch 9/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 12ms/step - accuracy: 0.8075 - loss: 0.4140 - val_accuracy: 0.8370 - val_loss: 0.3677\nEpoch 10/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8175 - loss: 0.3960 - val_accuracy: 0.8496 - val_loss: 0.3479\nEpoch 11/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 10ms/step - accuracy: 0.8335 - loss: 0.3696 - val_accuracy: 0.8521 - val_loss: 0.3354\nEpoch 12/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 10ms/step - accuracy: 0.8394 - loss: 0.3590 - val_accuracy: 0.8535 - val_loss: 0.3226\nEpoch 13/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 13ms/step - accuracy: 0.8496 - loss: 0.3385 - val_accuracy: 0.8735 - val_loss: 0.3024\nEpoch 14/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 12ms/step - accuracy: 0.8556 - loss: 0.3256 - val_accuracy: 0.8753 - val_loss: 0.2905\nEpoch 15/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8663 - loss: 0.3069 - val_accuracy: 0.8764 - val_loss: 0.2824\nEpoch 16/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8740 - loss: 0.2924 - val_accuracy: 0.8714 - val_loss: 0.2846\nEpoch 17/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 10ms/step - accuracy: 0.8749 - loss: 0.2875 - val_accuracy: 0.8708 - val_loss: 0.2804\nEpoch 18/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 10ms/step - accuracy: 0.8775 - loss: 0.2816 - val_accuracy: 0.8998 - val_loss: 0.2482\nEpoch 19/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 15ms/step - accuracy: 0.8829 - loss: 0.2722 - val_accuracy: 0.8973 - val_loss: 0.2459\nEpoch 20/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 10ms/step - accuracy: 0.8891 - loss: 0.2575 - val_accuracy: 0.9007 - val_loss: 0.2384\n\n\n\n\n\n# Evaluate model\nval_loss, val_acc = title_model.evaluate(val_ds.map(lambda x, y: (x[0], y)))\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\n\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 5ms/step - accuracy: 0.9026 - loss: 0.2368\nValidation Accuracy: 0.90\n\n\n\n\n\nWe will create a function that we can use for the rest of the models to plot Training and Validation Accuracy/Loss to keep track of how our models do throughout the epochs.\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Title-Only Model: Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Title-Only Model: Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualization for title model\nutils.plot_model(title_model, \"title_visualization.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\nThe title-only model achieving 90% validation accuracy is quite impressive, considering the fact that it is only using the title of the article. This suggests that the title alone must contain a lot of important information for distinguishing between real vs.¬†fake news. The model is effectively capturing key features from just a short, high-level summary of the content, which is a positive indicator of the model‚Äôs ability to generalize from minimal information.\n\n\n\n\n\n# Functional API for Text Model\n# Text Input Layer\ntext_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text_input\")\n\n# Shared vectorization and embedding layers\ntext_vectors = shared_vectorize_layer(text_input)\ntext_embeddings = shared_embedding(text_vectors)\n\n# Rest of the layers\n# Converts sequence into a single vector\nx = layers.GlobalAveragePooling1D()(text_embeddings)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\n# Output layer\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Define the model\ntext_model = keras.Model(inputs=text_input, outputs=output, name=\"Text_Only_Model\")\n\n\n# Compile the model\ntext_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\n\n# Train the Model\nhistory = text_model.fit(\n    train_ds.map(lambda x, y: (x[1], y)), # Extract text and label\n    validation_data=val_ds.map(lambda x, y: (x[1], y)), # Extract text and label\n    epochs=20,  # Adjust epochs based on overfitting\n    batch_size=100\n)\n\nEpoch 1/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 24ms/step - accuracy: 0.6196 - loss: 0.6552 - val_accuracy: 0.8546 - val_loss: 0.4592\nEpoch 2/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.8534 - loss: 0.4076 - val_accuracy: 0.8887 - val_loss: 0.2883\nEpoch 3/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 23ms/step - accuracy: 0.9047 - loss: 0.2692 - val_accuracy: 0.9310 - val_loss: 0.2111\nEpoch 4/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 28ms/step - accuracy: 0.9288 - loss: 0.2141 - val_accuracy: 0.9562 - val_loss: 0.1735\nEpoch 5/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 19ms/step - accuracy: 0.9405 - loss: 0.1852 - val_accuracy: 0.9616 - val_loss: 0.1502\nEpoch 6/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9486 - loss: 0.1616 - val_accuracy: 0.9622 - val_loss: 0.1378\nEpoch 7/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.9497 - loss: 0.1484 - val_accuracy: 0.9658 - val_loss: 0.1237\nEpoch 8/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 18ms/step - accuracy: 0.9554 - loss: 0.1360 - val_accuracy: 0.9622 - val_loss: 0.1193\nEpoch 9/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 20ms/step - accuracy: 0.9563 - loss: 0.1270 - val_accuracy: 0.9694 - val_loss: 0.1072\nEpoch 10/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 18ms/step - accuracy: 0.9607 - loss: 0.1166 - val_accuracy: 0.9703 - val_loss: 0.1013\nEpoch 11/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 24ms/step - accuracy: 0.9664 - loss: 0.1084 - val_accuracy: 0.9717 - val_loss: 0.0974\nEpoch 12/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.9664 - loss: 0.1034 - val_accuracy: 0.9726 - val_loss: 0.0929\nEpoch 13/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 18ms/step - accuracy: 0.9686 - loss: 0.0978 - val_accuracy: 0.9712 - val_loss: 0.0899\nEpoch 14/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 21ms/step - accuracy: 0.9691 - loss: 0.0926 - val_accuracy: 0.9726 - val_loss: 0.0859\nEpoch 15/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 18ms/step - accuracy: 0.9717 - loss: 0.0884 - val_accuracy: 0.9733 - val_loss: 0.0833\nEpoch 16/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 24ms/step - accuracy: 0.9730 - loss: 0.0840 - val_accuracy: 0.9733 - val_loss: 0.0815\nEpoch 17/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 18ms/step - accuracy: 0.9749 - loss: 0.0794 - val_accuracy: 0.9744 - val_loss: 0.0802\nEpoch 18/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 19ms/step - accuracy: 0.9764 - loss: 0.0756 - val_accuracy: 0.9751 - val_loss: 0.0774\nEpoch 19/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 21ms/step - accuracy: 0.9771 - loss: 0.0710 - val_accuracy: 0.9753 - val_loss: 0.0753\nEpoch 20/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 18ms/step - accuracy: 0.9775 - loss: 0.0681 - val_accuracy: 0.9742 - val_loss: 0.0751\n\n\n\n\n\n# Evaluate model\nval_loss, val_acc = text_model.evaluate(val_ds.map(lambda x, y: (x[1], y)))\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\n\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 16ms/step - accuracy: 0.9777 - loss: 0.0740\nValidation Accuracy: 0.97\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Text-Only Model: Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Text-Only Model: Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualization for title model\nutils.plot_model(text_model, \"text_visualization.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\nThe text-only model achieving a 97% validation accuracy is a sizable improvement over the title-only model‚Äôs 90%. This suggests that the full text provides more valuable information for distinguishing between classes like real vs.¬†fake news compared to just the title. The increase in accuracy means that the body of the text contains additional context, details, and subtle cues that the model can use to make more informed predictions.\n\n\n\n\n\n# Title Input\ntitle_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title_input\")\ntitle_vectors = shared_vectorize_layer(title_input)\ntitle_embeddings = shared_embedding(title_vectors)\ntitle_output = layers.GlobalAveragePooling1D()(title_embeddings)\n\n# Text Input\ntext_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text_input\")\ntext_vectors = shared_vectorize_layer(text_input)\ntext_embeddings = shared_embedding(text_vectors)\ntext_output = layers.GlobalAveragePooling1D()(text_embeddings)\n\n# Combinining inputs\nmerged = layers.concatenate([title_output, text_output])\n\n# Fully connected layers\nx = layers.Dense(64, activation=\"relu\")(merged)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\n# Output Layer\nfinal_output = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Define the model\ncombined_model = keras.Model(inputs=[title_input, text_input], outputs=final_output, name=\"Title_Text_Model\")\n\n\n# Compile Model\ncombined_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\n\n# Train Model\nhistory = combined_model.fit(\n    # Get title, text, and labels\n    train_ds.map(lambda x, y: ((x[0], x[1]), y)),\n    # Get title, text, and labels\n    validation_data=val_ds.map(lambda x, y: ((x[0], x[1]), y)),\n    epochs=20,\n    batch_size=100\n)\n\nEpoch 1/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 31ms/step - accuracy: 0.7980 - loss: 0.4460 - val_accuracy: 0.9634 - val_loss: 0.1102\nEpoch 2/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 25ms/step - accuracy: 0.9672 - loss: 0.1036 - val_accuracy: 0.9616 - val_loss: 0.1036\nEpoch 3/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 32ms/step - accuracy: 0.9703 - loss: 0.0873 - val_accuracy: 0.9676 - val_loss: 0.0893\nEpoch 4/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9803 - loss: 0.0649 - val_accuracy: 0.9742 - val_loss: 0.0736\nEpoch 5/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 31ms/step - accuracy: 0.9823 - loss: 0.0571 - val_accuracy: 0.9652 - val_loss: 0.0936\nEpoch 6/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 25ms/step - accuracy: 0.9817 - loss: 0.0545 - val_accuracy: 0.9753 - val_loss: 0.0707\nEpoch 7/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7s 40ms/step - accuracy: 0.9863 - loss: 0.0467 - val_accuracy: 0.9762 - val_loss: 0.0659\nEpoch 8/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 33ms/step - accuracy: 0.9860 - loss: 0.0423 - val_accuracy: 0.9836 - val_loss: 0.0548\nEpoch 9/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 30ms/step - accuracy: 0.9872 - loss: 0.0392 - val_accuracy: 0.9827 - val_loss: 0.0567\nEpoch 10/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 25ms/step - accuracy: 0.9854 - loss: 0.0412 - val_accuracy: 0.9665 - val_loss: 0.0922\nEpoch 11/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 33ms/step - accuracy: 0.9874 - loss: 0.0385 - val_accuracy: 0.9748 - val_loss: 0.0717\nEpoch 12/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 27ms/step - accuracy: 0.9849 - loss: 0.0417 - val_accuracy: 0.9861 - val_loss: 0.0494\nEpoch 13/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9910 - loss: 0.0277 - val_accuracy: 0.9845 - val_loss: 0.0515\nEpoch 14/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 31ms/step - accuracy: 0.9911 - loss: 0.0270 - val_accuracy: 0.9836 - val_loss: 0.0576\nEpoch 15/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9919 - loss: 0.0262 - val_accuracy: 0.9854 - val_loss: 0.0517\nEpoch 16/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 32ms/step - accuracy: 0.9935 - loss: 0.0215 - val_accuracy: 0.9847 - val_loss: 0.0541\nEpoch 17/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9932 - loss: 0.0223 - val_accuracy: 0.9834 - val_loss: 0.0547\nEpoch 18/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 25ms/step - accuracy: 0.9932 - loss: 0.0233 - val_accuracy: 0.9766 - val_loss: 0.0727\nEpoch 19/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 32ms/step - accuracy: 0.9909 - loss: 0.0256 - val_accuracy: 0.9843 - val_loss: 0.0562\nEpoch 20/20\n180/180 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5s 26ms/step - accuracy: 0.9934 - loss: 0.0219 - val_accuracy: 0.9840 - val_loss: 0.0565\n\n\n\n\n\nval_loss, val_acc = combined_model.evaluate(\n    val_ds.map(lambda x, y: ((x[0], x[1]), y))\n)\n\n# Print Validation Accuracy\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\n\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 14ms/step - accuracy: 0.9872 - loss: 0.0561\nValidation Accuracy: 0.98\n\n\n\n\n\n\n# Create figure\nplt.figure(figsize=(10,4))\n\n# Plot Training vs. Validation Accuracy\nplt.subplot(1,2,1)\nplt.plot(history.history[\"accuracy\"], label = \"Training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Accuracy\")\nplt.title(\"Combined Model: Training vs. Validation Accuracy\")\nplt.legend()\n\n# Plot Training vs. Validation Loss\nplt.subplot(1,2,2)\nplt.plot(history.history[\"loss\"], label = \"Training\")\nplt.plot(history.history[\"val_loss\"], label = \"Validation\")\nplt.gca().set(xlabel = \"Epoch\", ylabel = \"Loss\")\nplt.title(\"Combined Model: Training vs. Validation Loss\")\nplt.legend()\n\n# Show\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualization for title model\nutils.plot_model(combined_model, \"combined_visualization.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\nThe combined model achieving a 98% validation accuracy shows an improvement of 8% over the title-only model, but only a 1% improvement over the text-only model. This suggests that while combining the title and the full text provides some added value, the majority of the performance boost likely comes from the full text itself.\nThe large jump from the title-only model to the combined model reflects how the title can still contribute useful information, even though it may not be as rich as the full article. The fact that the improvement over the text-only model is only 1% implies that the full text already captures most of the necessary features for accurate classification, and adding the title does not introduce much additional useful information."
  },
  {
    "objectID": "posts/HW6_TextClassification/index.html#step-6-evaluating-model-performance-on-unseen-data",
    "href": "posts/HW6_TextClassification/index.html#step-6-evaluating-model-performance-on-unseen-data",
    "title": "Text Classification",
    "section": "",
    "text": "Now that we‚Äôve fine-tuned our model and optimized its performance on validation data, it‚Äôs time for the ultimate test: unseen data. Evaluating our best model on fresh, unencountered samples will help us see how well our model generalizes beyond the training set.\n\n\n\ntest_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_test.csv\"\n\nTo have consistent results, we‚Äôll preprocess this data using the same make_dataset function we defined earlier. This will standardize formatting, tokenize the text appropriately, and prepare it for evaluation.\n\n# Dataset\ntest_data = make_dataset(test_url)\n\n\n\n\n\n# Evaluate the model\ntest_loss, test_accuracy = combined_model.evaluate(test_data)\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n\n# Evaluate the model on validation data\nval_loss, val_accuracy = combined_model.evaluate(val_ds)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\n225/225 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 11ms/step - accuracy: 0.9842 - loss: 0.0576\nTest Accuracy: 0.9833\nTest Loss: 0.0595\n45/45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 11ms/step - accuracy: 0.9872 - loss: 0.0561\nValidation Accuracy: 0.9840\nValidation Loss: 0.0565\n\n\n\nweights = combined_model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = shared_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\n\nweights = combined_model.get_layer('embedding').get_weights()[0]\n\n\nprint(weights.shape)\n\n(2000, 16)\n\n\n\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\n\nweights.shape\n\n(2000, 2)\n\n\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nembedding_df\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n\n0.458991\n-0.369868\n\n\n1\n[UNK]\n-3.254118\n3.480346\n\n\n2\nsaid\n11.903173\n7.233503\n\n\n3\ntrump\n-7.662038\n3.243038\n\n\n4\nus\n13.215361\n-7.409301\n\n\n...\n...\n...\n...\n\n\n1995\nreaction\n-4.046322\n-2.003912\n\n\n1996\nministers\n3.829593\n-2.292025\n\n\n1997\nwonder\n-6.529799\n-4.318990\n\n\n1998\nsetting\n-0.664706\n0.334590\n\n\n1999\nsector\n1.703211\n0.525156\n\n\n\n\n2000 rows √ó 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\nThese words are all about structure, planning, and organization. ‚ÄúLaw‚Äù and ‚ÄúPolitics‚Äù are both about how society is run, dealing with rules, policies, and decisions. ‚ÄúOrganization‚Äù and ‚ÄúProjects‚Äù are about groups or actions focused on reaching specific goals through planning and teamwork. The word ‚Äúgoal‚Äù connects everything, since all the other words are about setting and achieving goals, whether it‚Äôs in politics, law, organizations, or projects. Their closeness shows they all involve systems or efforts to make things happen, which is why they are similar."
  },
  {
    "objectID": "posts/HW1_InteractiveGraphics/index.html",
    "href": "posts/HW1_InteractiveGraphics/index.html",
    "title": "Interactive Data Visualizations",
    "section": "",
    "text": "Introduction\nWelcome back to the Python series!\nHave you ever wondered how climate trends shift across different regions and time periods? In this blog post, we‚Äôll dive into NOAA climate data and bring it to life with interactive, eye-catching graphics. We will be starting off with learning how to manipulate data and create super cool, interactive data visualizations using Plotly. Along the way, we‚Äôll break down the process step by step‚Äîso whether you‚Äôre a data science pro or just getting started with visualization, you‚Äôll walk away with a solid understanding of how to turn raw data into compelling insights.\nExpect a mix of code, cool graphs, and clear explanations ‚Äî like the kind of post you wish you had when you first started working with data. Our goal? To make climate data not just informative, but also fun to explore. So, let‚Äôs get started!\n\n\n\nStep 1: Import the Necessary Libraries\nLet‚Äôs begin by importing all the libraries we need to start working on this project. We will see how they will be used throughout the tutorial.\n\n# Import libraries\nimport os\nimport sqlite3\nimport numpy as np\nimport pandas as pd\nimport urllib.request\nimport plotly.io as pio\nfrom plotly import express as px\nfrom sklearn.linear_model import LinearRegression\n\n\n\nStep 2: Create a Database\nBefore we can uncover temperature trends or create eye-catching visualizations, we need to start with the basics‚Äîstructuring our data properly. Think of it like organizing your closet: you wouldn‚Äôt just toss everything into one big pile (unless chaos is your thing). Instead, we‚Äôll neatly arrange our climate data into three separate tables:\ntemperatures ‚Äì where we store temperature readings over time  stations ‚Äì the locations where these temperatures were recorded  countries ‚Äì a reference table that links stations to actual countries \nThe goal? To create a well-organized database that makes it easy to analyze climate trends later on.\n\nSetting Up the Database\nWe‚Äôll start by creating a database and defining our three tables. To make sure our temperature readings are actually useful, we‚Äôll remove any rows with NaN (missing) values‚Äîbecause nothing ruins a good dataset like incomplete data!\nMake sure to keep column names exactly as they are and use the CSV file from lecture to populate the countries table. And of course, like any responsible coder, we‚Äôll close our database connection when we‚Äôre done.\nOnce this step is complete, we‚Äôll have a clean, structured database ready for exploration. Let‚Äôs get coding!\n\n\n1. Set Up Workspace, Create Intervals and Download CSV\nBefore diving into data analysis, we need to set up our workspace and retrieve the necessary data. First, we create a folder called HW1 to store our downloaded files. If the folder doesn‚Äôt already exist, we make one to keep things organized.\nNext, we define decade-based intervals from 1901 to 2019 in the format ‚Äú1901-1910‚Äù, ‚Äú1911-1920‚Äù, and so on. These intervals help us efficiently fetch climate data in chunks.\nFinally, we loop through each interval and download the corresponding CSV file from a GitHub repository. Each file is stored in our HW1 folder, ensuring we have all the temperature records neatly organized and ready for processing.\n\n# Show figures in Quarto \npio.renderers.default=\"iframe\"\n\n# Create a folder called HW1\nif not os.path.exists(\"HW1\"): \n    os.mkdir(\"HW1\")\n\n# Create intervals for data\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\n\n# Retrieve intervals\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"HW1/{interval}.csv\")\n\n\n\n2. Create a SQLite Database\nBefore we can start working with our climate data, we need a database to store everything. This line of code creates a new SQLite database called ‚Äútemps.db‚Äù if it doesn‚Äôt already exist. If the database is already there, it simply connects to it, allowing us to store, retrieve, and analyze temperature data efficiently.\n\n# Connect to database or create\nconn = sqlite3.connect(\"temps.db\")\n\n\n\n3. Prepare Dataframe\nWhen working with climate data, it‚Äôs often stored in a wide format, where each month‚Äôs temperature is in a separate column. However, for easier analysis and visualization, we need to convert it into a long format.\nThe prepare_df function does exactly that! It melts the DataFrame, restructuring it so that each row represents a single temperature reading with columns for station ID, year, month, and temperature. It also cleans up the ‚ÄúMonth‚Äù column, converting it from ‚ÄúVALUE1‚Äù to 1, ‚ÄúVALUE2‚Äù to 2, and so on. Finally, it scales the temperature values by dividing them by 100 to get proper decimal values.\nThis transformation makes our data much more user-friendly and ready for further analysis!\n\ndef prepare_df(df):\n    \"\"\"\n    Transforms a wide-format DataFrame into a long-format DataFrame by melting month-wise temperature values.\n\n    Parameters:\n    df (pd.DataFrame): A wide-format DataFrame containing columns \"ID\", \"Year\", and temperature values labeled \n                        as \"VALUE1\" to \"VALUE12\".\n\n    Returns:\n    df (pd.DataFrame): A long-format DataFrame with columns [\"ID\", \"Year\", \"Month\", \"Temp\"], where \"Month\" is \n                  an integer (1-12) and \"Temp\" is scaled by dividing by 100.\n    \"\"\"\n\n    # Melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # Cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n\n    return df\n\n\n\n4. Fetch Country CSV\nTo integrate country data into our database, we first fetch a CSV file containing country codes and names from a public URL. We then load it into a Pandas DataFrame, ensuring it‚Äôs clean by dropping any missing values. Finally, we store the DataFrame in our SQL database as a table named ‚Äúcountries‚Äù, replacing any existing version of the table if necessary. Now, our database has a reliable reference for country names and codes!\n\n# URL of the countries CSV file\ncountries_csv = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n\n# Read CSV into a dataframe\ncountries_df = pd.read_csv(countries_csv)\n\n# Ensure no NaN values\ncountries_df.dropna(inplace=True) \n\n# Convert dataframe into a SQL database table named \"countries\"\n# If the table already exists, it will be replaced\ncountries_df.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n278\n\n\n\n\nFetch Station CSV\nLet‚Äôs do the same for stations data!\n\n# URL of the stations CSV file\nstations_csv = \"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\n\n# Read CSV into a dataframe\nstations_df = pd.read_csv(stations_csv)\n\n# Ensure no NaN values\nstations_df.dropna(inplace=True)  \n\n# Convert dataframe into a SQL database table named \"stations\"\n# If the table already exists, it will be replaced\nstations_df.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\nWe loop through each temperature dataset by decade, reading the CSV files into Pandas DataFrames. Each dataset is transformed from wide format to long format for easier analysis and then stored in our SQL database under the ‚Äútemperatures‚Äù table. Instead of replacing existing data, we append new records to ensure we capture all decades.\nOnce all temperature data is loaded, we merge station data with country data by extracting country codes from station IDs. This step allows us to connect temperature readings to their respective countries, making our dataset even more informative!\n\n# Iterate over the list of intervals to process and store temperature data\nfor interval in intervals:\n    # Construct the file path for the current interval's CSV file\n    filepath = f\"HW1/{interval}.csv\"\n    # Read the CSV file into a Pandas DataFrame\n    df = pd.read_csv(filepath)\n    # Transform the DataFrame from wide format to long format\n    df = prepare_df(df)  \n    # Store the transformed data into the \"temperatures\" table in the SQL database\n    # If the table exists, append new data without replacing existing records\n    df.to_sql(\"temperatures\", conn, if_exists=\"append\", index=False)\n\n# After loading all temperature data, merge station data with country data\n# Get the \"FIPS 10-4\" country code from the first two characters of the station ID\nstations_df[\"FIPS 10-4\"] = stations_df[\"ID\"].str[:2]\n# Merge stations with countries\ndf = pd.merge(stations_df, countries_df, on=\"FIPS 10-4\") \n\n\n\n\nStep 3: Write a Query Function\nIn this step, we‚Äôll be building a function called query_climate_database() that helps us retrieve specific temperature data from a database. Think of it like a search tool where you can ask for temperature data based on a particular country, a date range, and even the month. This function takes five arguments: - the database file name (db_file) - the name of the country you‚Äôre interested in (country) - the start year (year_begin) - the end year (year_end) - the specific month (month) you want to focus on.\nOnce we pass all these details into the function, it constructs a SQL query that asks the database for the exact information we need. The query will return a Pandas DataFrame with columns like NAME (the station‚Äôs name), LATITUDE and LONGITUDE (coordinates of the station), Country (the name of the country), Year and Month (the time the data was recorded), and Temp (the average temperature at the station during that month). Since the temperature data is already averaged by month, no further aggregation is needed.\nWe use Python‚Äôs f-strings in the SQL query to dynamically insert the parameters we‚Äôve passed to the function, making it easy to fetch data based on whatever year, country, and month you specify. By the end of this, you‚Äôll have a powerful tool to pull temperature data exactly how you need it!\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Queries through a climate database to get temperature readings for a specified country, \n    within a given year range, and for a specific month.\n\n    Parameters:\n    db_file : str\n        The file path to the SQLite database.\n    country : str\n        The name of the country for which temperature data is needed.\n    year_begin : int\n        The start year of the query range (inclusive).\n    year_end : int\n        The end year of the query range (inclusive).\n    month : int\n        The month (1-12) for which temperature data is needed.\n\n    Returns:\n    pandas.DataFrame\n        A DataFrame containing temperature readings with the following columns:\n        - NAME: The station name.\n        - LATITUDE: The latitude of the station.\n        - LONGITUDE: The longitude of the station.\n        - Country: The country where the station is located.\n        - Year: The year of the reading.\n        - Month: The month of the reading.\n        - Temp: The average temperature recorded at the station for the given month and year.\n\n    \"\"\"\n    # Connect to db_file provided\n    conn = sqlite3.connect(db_file)\n    # Create a cursor\n    cursor = conn.cursor()\n\n    # Query that gets the columns and value needed to create the end df\n    query = f\"\"\"\n            SELECT \n                s.NAME, s.LATITUDE, s.LONGITUDE, c.Name AS Country, \n                t.Year, t.Month, AVG(t.Temp) AS Temp\n            FROM temperatures t\n            JOIN stations s ON t.ID = s.ID\n            JOIN countries c ON SUBSTR(s.ID, 1, 2) = c.\"FIPS 10-4\"\n            WHERE c.Name = ? \n              AND t.Year BETWEEN ? AND ? \n              AND t.Month = ?\n              AND t.Temp IS NOT NULL\n            GROUP BY s.NAME, s.LATITUDE, s.LONGITUDE, c.Name, t.Year, t.Month;\n            \"\"\"\n    \n    # Create the df with the query and parameters \n    df = pd.read_sql_query(query, conn, params=(country, year_begin, year_end, month))\n\n    # Close connection\n    conn.close()\n\n    # Return final dataframe\n    return df\n\n\n\nStep 4: Write a Geographic Scatter Function for Yearly Temperature Increases.\nIn this part, we‚Äôll create a function called temperature_coefficient_plot() that generates an interactive map to visualize how the average yearly temperature change varies across stations in a given country.\nThe function accepts six main arguments: - db_file - country - year_begin - year_end - month - min_obs ‚Äî with min_obs filtering out stations with less data than the specified threshold.\nUsing Plotly Express, the function calculates the yearly temperature change for each station via linear regression, and then plots a scatter map with points colored based on temperature change.\nThe coef() function calculates the slope (or coefficient) of a linear regression model that predicts how much the temperature at a given station is changing on average each year.\n\ndef coef(data_group):\n    \"\"\"\n    Calculates the coefficient (slope) of a linear regression model that predicts temperature \n    based on the year for a given group of data.\n\n    Parameters:\n    data_group (pd.DataFrame): A DataFrame containing the columns \"Year\" and \"Temp\".\n\n    Returns:\n    float: The coefficient (slope) of the linear regression model.\n    \"\"\"\n    # Get \"Year\" - using double brackets to keep it a DataFrame\n    x = data_group[[\"Year\"]] \n    # Get \"Temp\"  - using a single bracket to keep it a Series\n    y = data_group[\"Temp\"] \n    # Initialize a Linear Regression Model\n    LR = LinearRegression()\n    # Fit the model to the data\n    LR.fit(x, y)\n    # Return the calculated coefficient from the regression model\n    return LR.coef_[0]\n\nThe combine_datasets() function takes in two DataFrames, df and result, and combines them into a new DataFrame (to_graph) for visualization. It aligns station data, including station names and coordinates, with their corresponding estimated yearly temperature change from the result DataFrame. The final result is a table ready for plotting, containing columns like ‚ÄúNAME‚Äù, ‚ÄúLATITUDE‚Äù, ‚ÄúLONGITUDE‚Äù, and ‚ÄúEstimated Yearly Increase (¬∞C)‚Äù.\nThis new dataset can be used to create interactive visualizations that show temperature trends across different stations.\n\ndef combine_datasets(df, result):\n    \"\"\"\n    This function iterates through the input DataFrame `df` and aligns it with the `result` \n    DataFrame. It constructs a new DataFrame (`to_graph`) that includes station names,\n    coordinates, and the estimated yearly temperature change.\n\n    Parameters:\n    df (pd.DataFrame): A DataFrame containing station information, including \"NAME\", \"LATITUDE\", \n                       \"LONGITUDE\", and \"year_count\" (the number of years the station has data for).\n    result (pd.Series or pd.DataFrame): A dataset containing the estimated yearly temperature increase \n                                        for each station.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with columns [\"NAME\", \"LATITUDE\", \"LONGITUDE\", \n                  \"Estimated Yearly Increase (¬∞C)\"], ready for visualization.\n    \"\"\"\n    # Create an empty DataFrame with the required columns for visualization\n    to_graph = pd.DataFrame(columns = [\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Estimated Yearly Increase (¬∞C)\"])\n    # Initialize indices for iterating through both input datasets\n    df_index = 0\n    result_index = 0\n    # Iterate through the station dataset to align it with the results\n    while( df_index &lt; df.shape[0]):\n        # Get the current station row\n        df_row = df.iloc[df_index]\n        # Append a new row to `to_graph` with station details and its estimated temperature increase\n        to_graph.loc[len(to_graph)] = {\"NAME\":df_row[\"NAME\"], \"LATITUDE\":df_row[\"LATITUDE\"], \"LONGITUDE\":df_row[\"LONGITUDE\"], \"Estimated Yearly Increase (¬∞C)\":result.iloc[result_index]}\n        # Move to the next station by skipping the number of years it represents\n        df_index += df_row[\"year_count\"]\n        # Move to the next result index\n        result_index += 1\n    \n    return to_graph\n\nThe temperature_coefficient_plot() function visualizes yearly temperature increases across climate stations in a specific country, over a given time period, and for a chosen month. It queries the climate database, filters out stations with insufficient data, and calculates the yearly temperature change for each station using linear regression. The resulting data is structured for visualization, including station names, coordinates, and the estimated temperature increase.\nUsing Plotly, the function generates an interactive scatter map where each point represents a station, color-coded by its estimated temperature change. This map can be customized with parameters for map styling, making it an easy and fast way to visualize climate trends across different regions!\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Generates a scatter map visualization of estimated yearly temperature increases \n    for climate stations in a given country over a specified time period.\n\n    Parameters:\n        db_file (str): Database file.\n        country (str): Country name.\n        year_begin (int): Start year of the analysis period.\n        year_end (int): End year of the analysis period.\n        month (int): Month for which temperature data should be analyzed.\n        min_obs (int): Minimum number of yearly observations required per station.\n        **kwargs: Additional keyword arguments for customizing the map visualization.\n\n    Returns:\n        plotly.graph_objects.Figure: A scatter mapbox figure visualizing the estimated \n                                     yearly temperature increase for climate stations.\n    \"\"\"\n    # Query the climate database for temperature data within the specified constraints\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Count the number of years each station has data for\n    df[\"year_count\"] = df.groupby(\"NAME\")[\"Year\"].transform(\"count\")\n    # Filter out stations with fewer observations than the minimum required\n    df = df[df[\"year_count\"] &gt;= min_obs]\n\n    # Apply the linear regression coefficient calculation for each station and month\n    result = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    # Round the estimated yearly temperature increase values for better readability\n    result = np.round(result, 4)\n\n    # Add the estimated yearly temperature increase as a new column in the DataFrame\n    df[\"Estimated Yearly Increase (¬∞C)\"] = result.reset_index(level=[\"NAME\", \"Month\"], drop=True)\n\n    # Combine the datasets to structure the data for visualization\n    to_graph = combine_datasets(df, result)\n\n    # Create a scatter map using Plotly to visualize the estimated temperature increases\n    fig = px.scatter_mapbox(to_graph,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        color = \"Estimated Yearly Increase (¬∞C)\",\n                        hover_name = \"NAME\",\n                        **kwargs\n                        )\n    # Adjust layout to remove unnecessary margins\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    return fig\n\nFor example, you could use this function to visualize temperature changes across India between 1980-2020 during January, helping you see which areas are warming the most.\n\n# Set a colormap\ncolor_map = px.colors.diverging.RdGy_r \n\n# Call the function temperature_coefficient_plot \nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n#Show the plot\nfig.show()\n\n\n\n\nAnd then, here is another example from China in the time range 1990-2010.\n\n# Set a colormap\ncolor_map = px.colors.diverging.RdGy_r \n\n# Call the function temperature_coefficient_plot \nfig = temperature_coefficient_plot(\"temps.db\", \"China\", 1990, 2010, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n#Show the plot\nfig.show()\n\n\n\n\n\n\nStep 5: Create An Additional Query Function\nIn this step, we will create another query function that is different from the one we made. The query_latitude_temp() function queries a climate database to retrieve the average temperature change by latitude band over specified decades for a given country. The query groups data by country, latitude bands (rounded to the nearest 5 degrees), and decade, then calculates the average temperature for each group within the specified year range.\nThe function returns a Pandas DataFrame containing columns for country, latitude band, decade, and average temperature. This makes it easy to analyze temperature trends across different latitude bands and decades, providing insights into how temperature change varies geographically and temporally within a specific country.\n\ndef query_latitude_temp(db_file, country, decade_begin, decade_end):\n    \"\"\"\n    Queries a database to retrieve the average temperature change by latitude band \n    across different decades for a specific country.\n\n    Args:\n        db_file (str): Database.\n        country (str): The name of the country for which to retrieve the temperature data.\n        decade_begin (int): The beginning year of the range.\n        decade_end (int): The ending year of the range.\n\n    Returns:\n        pd.DataFrame: A dataframe containing the country, latitude bands, decades, and \n                      average temperatures.\n    \"\"\"\n    \n    # Connect to db_file provided\n    conn = sqlite3.connect(db_file)\n    # Create a cursor\n    cursor = conn.cursor()\n\n    # SQL query to retrieve the required data\n    # The query joins the 'temperatures', 'stations', and 'countries' tables,\n    # groups by country, latitude bands (rounded to nearest 5 degrees), and decade,\n    # and calculates the average temperature for each group\n    query = f\"\"\"\n            SELECT \n                s.Name, \n                ROUND(s.LATITUDE / 5.0) * 5 AS Latitude_Band, \n                c.Name AS Country,\n                (t.Year / 10) * 10 AS Decade, \n                AVG(t.Temp) AS Avg_Temp\n            FROM temperatures t\n            JOIN stations s ON t.ID = s.ID\n            JOIN countries c ON SUBSTR(s.ID, 1, 2) = c.\"FIPS 10-4\"\n            WHERE c.Name = ?  \n              AND t.Year BETWEEN ? AND ?  \n              AND t.Temp IS NOT NULL\n            GROUP BY Country, Latitude_Band, Decade\n            ORDER BY Country, Latitude_Band, Decade;\n            \"\"\"\n    \n    # Create the df with the query and parameters \n    df = pd.read_sql_query(query, conn, params=(country, decade_begin, decade_end))\n\n    # Close connection\n    conn.close()\n\n    # Return final dataframe\n    return df\n\n\n\nStep 6: Create an Interactive Heatmap\nA great way to use the new query function is by creating an interactive heatmap that visualizes temperature changes across latitude bands and decades for a specific country! We‚Äôll walk you through how to do this using Plotly.\nFirst, we query the climate database to retrieve the average temperature data by latitude band and decade. We then format the data for Plotly‚Äôs px.imshow() function, which generates the heatmap. Each cell‚Äôs color represents the average temperature for a specific latitude band and decade, offering a clear visual of temperature trends over time. The interactive map allows users to hover over the cells for detailed temperature data, making it easy to spot long-term patterns and insights.\n\ndef latitude_plot_heatmap(db_file, country, decade_begin, decade_end, **kwargs):\n    \"\"\"\n    Plots an interactive heatmap to visualize temperature trends by latitude band and decade for a specific country.\n\n    Args:\n        db_file (str): The file path of the database containing temperature data.\n        country (str): The name of the country for which to plot the temperature trends.\n        decade_begin (int): The beginning year of the range.\n        decade_end (int): The ending year of the range.\n        **kwargs: Additional keyword arguments to customize the plot (not used in the current version).\n\n    Returns:\n        None: The function generates and displays an interactive heatmap using Plotly.\n    \"\"\"\n    # Query temperature data for the specified country and decade range\n    df = query_latitude_temp(db_file, country, decade_begin, decade_end)\n\n    # Pivot the DataFrame and sort by the latest decade's temperatures in descending order\n    df_grouped = df.pivot(index=\"Latitude_Band\", columns=\"Decade\", values=\"Avg_Temp\")\n    df_grouped = df_grouped.sort_values(by=df_grouped.columns[-1], ascending=False)\n    # Round the temperature values to 4 decimal places\n    df_grouped = np.round(df_grouped, 4)\n\n    # Convert Latitude_Band index to a column\n    df_grouped = df_grouped.reset_index()  \n    # Convert column names to strings\n    df_grouped.columns = df_grouped.columns.astype(str)  \n\n    # Sort Latitude_Band in ascending order to ensure proper display\n    df_grouped[\"Latitude_Band\"] = pd.to_numeric(df_grouped[\"Latitude_Band\"], errors='coerce')\n    df_grouped = df_grouped.sort_values(by=\"Latitude_Band\", ascending=True)\n\n    # Melt the DataFrame into long format\n    df_grouped = df_grouped.melt(id_vars=\"Latitude_Band\", var_name=\"Decade\", value_name=\"Avg_Temp\")\n    df_grouped[\"Decade\"] = df_grouped[\"Decade\"].astype(int)    \n    \n    \n    fig = px.imshow(\n        df_grouped.pivot(index=\"Latitude_Band\", columns=\"Decade\", values=\"Avg_Temp\"),\n        color_continuous_scale=\"RdYlBu_r\", \n        title=f\"Temperature Trends by Latitude and Decade in {country}\"\n    )\n    \n    \n    # Customize the layout to improve plot interactivity and appearance\n    fig.update_layout(\n                      yaxis_title=\"Latitude Band\",\n                      yaxis=dict(autorange=\"reversed\"),\n                      coloraxis_colorbar=dict(title=\"Avg Temp (¬∞C)\"),\n                      hovermode=\"closest\"\n                     )\n    \n    fig.update_traces(hovertemplate=\"Decade: %{x}&lt;br&gt;Latitude Band: %{y}&lt;br&gt;Average Temp: %{z}¬∞C\")\n\n    fig.show()\n\n\ndf_grouped = latitude_plot_heatmap(\"temps.db\", \"India\", 1980, 2020, \n                                  zoom = 2,\n                                  mapbox_style=\"carto-positron\",\n                                  color_continuous_scale=color_map\n                                  )\n\n\n\n\n\n\nStep 7: Create an Interactive Barplot\nThis next function creates an interactive bar chart using Plotly. You get a clear, colorful picture of how temperatures have shifted over time ‚Äî all without drowning in rows of raw data.\nWhat makes the plot visually appealing is the smart use of color gradients to highlight changes in temperature. The plot includes decade-wise subplots with clean, shared axis labels for easy interpretation. No clutter here ‚Äî just simple, intuitive visual storytelling.\n\ndef latitude_plot_bar(db_file, country, decade_begin, decade_end, **kwargs):\n    \"\"\"\n    Generates a faceted bar chart to visualize the average temperature across different latitude bands and decades.\n\n    Args:\n        db_file (str): The database file path.\n        country (str): The name of the country for which to plot the temperature trends.\n        decade_begin (int): The starting decade of the range.\n        decade_end (int): The ending decade of the range.\n        **kwargs: Additional keyword arguments to customize the plot (e.g., width, height, labels).\n\n    Returns:\n        None: Displays an interactive faceted bar chart using Plotly.\n    \"\"\"\n    # Query temperature data for the specified country and decade range\n    df = query_latitude_temp(db_file, country, decade_begin, decade_end)\n\n    # Safeguard\n    if df.empty:\n        print(f\"No temperature data available for {country} in the specified range.\")\n        return\n    \n    # Filter data to keep only rows within the specified decade range\n    df_decade = df[(df[\"Decade\"] &gt;= decade_begin) & (df[\"Decade\"] &lt;= decade_end)]\n\n    # Group by Decade and Latitude Band to get average temperature\n    df_grouped = df_decade.groupby([\"Decade\", \"Latitude_Band\"], as_index=False)[\"Avg_Temp\"].mean()\n    df_grouped[\"Avg_Temp\"] = df_grouped[\"Avg_Temp\"].round(4)\n    \n    # Create the faceted bar chart\n    fig = px.bar(df_grouped, \n                 x=\"Latitude_Band\", \n                 y=\"Avg_Temp\", \n                 facet_col=\"Decade\", \n                 color=\"Avg_Temp\",\n                 color_continuous_scale=\"RdYlBu_r\",\n                 title=f\"Temperature by Latitude Across Decades in {country}\",\n                 facet_col_wrap=3,\n                 **kwargs)\n\n    fig.update_layout(\n        xaxis_title=\"Latitude Band\",  # Global x-axis title\n        yaxis_title=\"Avg Temperature (¬∞C)\",  # Global y-axis title\n        coloraxis_colorbar=dict(title=\"Avg_Temp (¬∞C)\"),\n        margin=dict(t=50, b=50, l=50, r=50)\n    )\n\n    # Rename subplot facet titles properly\n    fig.for_each_annotation(lambda ann: ann.update(text=f\"Decade: {ann.text.split('=')[-1]}\"))\n\n    fig.show()\n\n\nfig = latitude_plot_bar(\"temps.db\", \n                        \"India\", \n                        1980, \n                        2020)\n\n\n\n\n\n\nConclusion\nData visualization is a powerful way to uncover patterns, and with just a few lines of code, you can tell an engaging story. Whether you‚Äôre a data enthusiast or a climate scientist, this tool gives you a dynamic way to spot trends and patterns at a glance. Plug in your data, sit back, and watch your story unfold, all in an easy to read and intuitive format!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto preview myblog",
    "section": "",
    "text": "Text Classification\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\nHW1_InteractiveGraphics\n\n\nHW2_Scrapy\n\n\nHW3_WebDevelopment\n\n\nHW4_HeatDiffusion\n\n\nHW5_ImageClassification\n\n\nHW6_TextClassification\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\nHW1_InteractiveGraphics\n\n\nHW2_Scrapy\n\n\nHW3_WebDevelopment\n\n\nHW4_HeatDiffusion\n\n\nHW5_ImageClassification\n\n\nHW6_TextClassification\n\n\n\n\n\n\n\n\n\nMar 1, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Diffusion\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\nHW1_InteractiveGraphics\n\n\nHW2_Scrapy\n\n\nHW3_WebDevelopment\n\n\nHW4_HeatDiffusion\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nLights, Camera, Scrapy!\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\nHW1_InteractiveGraphics\n\n\nHW2_Scrapy\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Development\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\nHW1_InteractiveGraphics\n\n\nHW2_Scrapy\n\n\nHW3_WebDevelopment\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Data Visualizations\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\nHW1_InteractiveGraphics\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\nAbout\n\n\nWelcome\n\n\nHW0_PenguinVisualization\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Anahita‚Äôs Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 19, 2025\n\n\nAnahita Vaidhya\n\n\n\n\n\n\nNo matching items"
  }
]